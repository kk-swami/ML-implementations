{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Information theory, Relative entropy , mutual information and KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information theory is the branch of CS which deals  with quantification, storage and communication of digital information.\n",
    "\n",
    "\n",
    "A key measure to quantify information in information theory is entropy\n",
    "\n",
    "This builds off a basic premise of information theory, that the value/information content of a communicated message (event in statistics language) is higher, the less probable the event\n",
    "\n",
    "If the event is very probably, there is hardly any new information, whereas if the event is less probable, there is a lot of information\n",
    "\n",
    "In concrete terms, the Information of an event E \n",
    "\n",
    "I(E) is inversely proportional to the probability of the event p(E)\n",
    "\n",
    "In reality, the log(1/p) equation is used for this relation\n",
    "\n",
    "ie $I(E) = log\\frac{1}{p(E)}$  which is equivalent to -log(p(E))\n",
    "\n",
    "Why is this functional form used ? [See reasons here](https://stats.stackexchange.com/questions/87182/what-is-the-role-of-the-logarithm-in-shannons-entropy#:~:text=We%20can%20call%20log(1,events%20we%20can%20tell%20apart).)\n",
    "\n",
    "\n",
    "( In short , two reasons -\n",
    "1) a mathematical reason : logarithm is to make it growing linearly with system size and \"behave like information\". It is also to satisfy nice mathematical properties. For example, we would expect the entropy of tossing a coin n times to be n*entropy of tossing a coin once.\n",
    "\n",
    "Having a logarithm in the definition makes this mathematically possibe (check link above)\n",
    "\n",
    "2) An intuitive reason : if all events happen with the same probability p (such as a fair coin or a fair die - uniform distribution), the number of events is 1/p. For capturing 1/p events in binary notation, we have to use $log_{2}(1/p)$ bits\n",
    "This extends even in a non-uniform distribution because of theoretical reasons (see link above)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a fair coin, with probabilty of each event = 1/2,  what is information of the event ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information of event E in a fair coin toss is 1.0 bits\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "information = log2(1/(1/2))\n",
    "print(\"information of event E in a fair coin toss is {} bits\".format(information))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a fair die, with probabilty of each event = 1/6,  what is information of the event ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information of event E in a fair die toss is 2.584962500721156 bits\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "information = log2(1/(1/6))\n",
    "print(\"information of event E in a fair die roll is {} bits\".format(information))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, we see that since a p(E) for a die is 1/6 (lower than p(E) for a coin),\n",
    "information of event in a die roll is higher than coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy in information theory is analogous to the concept of entropy in thermodynamic systems\n",
    "\n",
    "Specifically, the entropy of a random variable X is the [average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's outcomes](https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "Note that entropy is defined for a random variable, whereas information is defined for an event\n",
    "\n",
    "Entropy is nothing but the expectation of information\n",
    "\n",
    "ie H(X) = E(I(X)) =  $\\sum_{i}p(Ei)log(\\frac{1}{p(E(i))})$\n",
    "\n",
    "This is the equation of Shannon Entropy, over events/states of the RV . This is analogous to the equation of Gibbs entropy in thermodynamics over states of the thermodynamical system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you compare entropy of tossing a fair coin vs a biased coin where p(H) = 0.7 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def entropy(prob_vec):\n",
    "    prob_vec = np.array(prob_vec)\n",
    "    return np.sum(-prob_vec*np.log2(prob_vec))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of a fair coin is 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of a fair coin is {} bits\".format(entropy([0.5, 0.5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of a biased coin with p(H) = 0.7  is 0.8812908992306927\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of a biased coin with p(H) = 0.7  is {} bits\".format(entropy([0.7, 0.3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that entropy of fair coin is larger, in fact , a uniform distribution has the highest entropy among all distributions, because there is more uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional entropy of RV Y|X  (represented as H(Y|X)) is the amount of information (think of it as bits) needed to describe Y given another RV X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that H(Y) = E(I(Y)) =  $\\sum_{i}p(Ei)log(\\frac{1}{p(E(i))})$ over all Events Ei in Y\n",
    "\n",
    "which can be equivalently written as $\\sum_{y}p(Y=y)log(\\frac{1}{p(Y=y)})$\n",
    "over all outcomes y of RV Y\n",
    "in a discete case\n",
    "\n",
    "\n",
    "Assume that RV X takes a specific value x\n",
    "\n",
    "Then, we can say as an extension that \n",
    "H(Y|X=x) = $\\sum_{y}p(Y=y|X=x)log(\\frac{1}{p(Y=y|X=x)})$\n",
    "\n",
    "Averaging over all possible values of X,\n",
    "we can define\n",
    "\n",
    "H(Y|X) = $\\sum_{x}p(X=x)H(Y|X=x)$ \n",
    " = $\\sum_{x}p(X=x) (\\sum_{y}p(Y=y|X=x)log(\\frac{1}{p(Y=y|X=x)}))$ \n",
    "  = $\\sum_{x}\\sum_{y}p(X=x,Y=y)log(\\frac{1}{p(Y=y|X=x)}))$ \n",
    "  \n",
    "  = $\\sum_{x,y}p(X=x,Y=y) log(\\frac{p(X=x)}{p(X=x, Y=y)})) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties of conditional entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Property 1 - If X and Y are independent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X and Y are independent RV, intuitively, H(Y|X) = H(Y)\n",
    "\n",
    "This can also be seen from the equation above. If X and Y are independent,\n",
    "p(X=x, Y=y) = p(X)p(Y)\n",
    "\n",
    "H(Y|X) = $\\sum_{x,y}p(X=x)*p(Y=y) log(\\frac{p(X=x)}{p(X=x)*p( Y=y)})) $\n",
    "\n",
    "= $\\sum_{x,y}p(X=x)*p(Y=y)log(\\frac{1}{p(Y=y)})$\n",
    "\n",
    "= $\\sum_{y}p(Y=y)log(\\frac{1}{p(Y=y)})$ = H(Y)\n",
    "\n",
    "\n",
    "This also implies that H(X,Y) = H(X) + H(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Property 2 - when is conditional entropy 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If H(Y|X)=0, it means Y is completely dependent on X / completely determined by X (ie P(X,Y) = P(X)), which can also be seen from the equation above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Property 3 (Chain Rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(Y|X) = H(Y,X) - H(X)\n",
    "\n",
    "Trivial from equation above \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Property 4 - generalization of chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, H(X1,X2...Xn) = H(X1) + H(X2|X1) + H(X3|X1,X2).... (generalizing equation above)\n",
    "\n",
    "Kind of analogous to chain rule of probability of random variables, except that is multiplicative instead of additive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Property 5 - Bayes rule of entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(Y|X) = H(X|Y) + H(Y) - H(X)\n",
    "\n",
    "Proof : \n",
    "H(Y|X) = H(Y,X) - H(X) = H(X,Y) - H(Y)\n",
    "H(Y|X) + H(X) = H(X|Y) + H(Y) = H(X,Y)\n",
    "=> H(Y|X) = H(X|Y) + H(Y) - H(X)\n",
    "\n",
    "Again, kind of analogous to Bayes theorem of probability , except that is multiplicative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Property 6 - H(Y|X) <= H(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof : \n",
    "\n",
    "Define relative entropy I(Y|X) = H(Y)-H(Y|X)\n",
    "\n",
    "Prove that I(Y|X) >= 0\n",
    "\n",
    "\n",
    "H(Y) =  -$\\sum_{y}p(Y=y)log({p( Y=y)})) $   \n",
    "H(Y|X) = $\\sum_{x,y}p(X=x,Y=y) log(\\frac{p(X=x)}{p(X=x, Y=y)})) $\n",
    "\n",
    "\n",
    "\n",
    "I =  -$\\sum_{y}p(Y=y)log({p( Y=y)})) $    - $\\sum_{x,y}p(X=x,Y=y) log(\\frac{p(X=x)}{p(X=x, Y=y)})) $\n",
    "\n",
    "\n",
    "\n",
    "$\\sum_{y}p(Y=y)log({p( Y=y)})) $ =  $\\sum_{x,y}p(X=x, Y=y)log({p( Y=y)})) $\n",
    "\n",
    "\n",
    "Therefore, I  = -  $\\sum_{x,y}p(X=x, Y=y)log({p( Y=y)})) $ - $\\sum_{x,y}p(X=x,Y=y) log(\\frac{p(X=x)}{p(X=x, Y=y)})) $\n",
    "\n",
    "\n",
    "= - ($\\sum_{x,y} (p(X=x, Y=y)log({p( Y=y)}) + p(X=x,Y=y) log(\\frac{p(X=x)}{p(X=x, Y=y)})))$)\n",
    "\n",
    "= $\\sum_{x,y} (p(X=x, Y=y)*log(\\frac{p(X=x,Y=y)}{p(X=x)*p(Y=y)})))$\n",
    "\n",
    "\n",
    "Multiplying and dividing numerator and denominator by p(X=x)*p(Y=y), we get\n",
    "\n",
    "\n",
    "I = $\\sum_{x,y} p(X=x)*p(Y=y)  * \\frac{(p(X=x, Y=y)}{p(X)*p(Y)}*log(\\frac{p(X=x,Y=y)}{p(X=x)*p(Y=y)})))$\n",
    "\n",
    "\n",
    "\n",
    "By Jensen's inequality, for convex functions, E(f(X)) >= f(E(X))\n",
    "\n",
    "xlogx is a convex function for x > 0\n",
    "\n",
    "Therefore, I =  $\\sum_{x,y} p(X=x)*p(Y=y)  *  f(\\frac{(p(X=x, Y=y)}{p(X)*p(Y)})$\n",
    "\n",
    "where f(z) = zlogz\n",
    "\n",
    "\n",
    ">=  $  f (\\sum_{x,y} p(X=x)*p(Y=y)  *  \\frac{(p(X=x, Y=y)}{p(X)*p(Y)})) $ \n",
    "\n",
    ">= f(1) >=0\n",
    "\n",
    "Hence Proved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) https://machinelearningmastery.com/what-is-information-entropy/  \n",
    "2) https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "3) https://stats.stackexchange.com/questions/87182/what-is-the-role-of-the-logarithm-in-shannons-entropy#:~:text=We%20can%20call%20log(1,events%20we%20can%20tell%20apart).\n",
    "4) https://machinelearningmastery.com/information-gain-and-mutual-information/\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
