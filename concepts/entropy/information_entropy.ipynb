{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Information theory, Relative entropy , mutual information and KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information theory is the branch of CS which deals  with quantification, storage and communication of digital information.\n",
    "\n",
    "\n",
    "A key measure to quantify information in information theory is entropy\n",
    "\n",
    "This builds off a basic premise of information theory, that the value/information content of a communicated message (event in statistics language) is higher, the less probable the event\n",
    "\n",
    "If the event is very probably, there is hardly any new information, whereas if the event is less probable, there is a lot of information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hartley's measure of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$I(E) = log_bL$\n",
    "\n",
    "where L is the number of possible unique values of RV X from which E is sampled\n",
    "\n",
    "Disadvantage : Does not take probability distributions of RV into account, kind of assumes all events are equally probable (uniform distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shannon's measure of information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In concrete terms, the Information of an event E \n",
    "\n",
    "I(E) is inversely proportional to the probability of the event p(E)\n",
    "\n",
    "In reality, the log(1/p) equation is used for this relation\n",
    "\n",
    "ie $I(E) = log\\frac{1}{p(E)}$  which is equivalent to -log(p(E))\n",
    "\n",
    "Why is this functional form used ? [See reasons here](https://stats.stackexchange.com/questions/87182/what-is-the-role-of-the-logarithm-in-shannons-entropy#:~:text=We%20can%20call%20log(1,events%20we%20can%20tell%20apart).)\n",
    "\n",
    "\n",
    "( In short , two reasons -\n",
    "1) a mathematical reason : logarithm is to make it growing linearly with system size and \"behave like information\". It is also to satisfy nice mathematical properties. For example, we would expect the entropy of tossing a coin n times to be n*entropy of tossing a coin once.\n",
    "\n",
    "Having a logarithm in the definition makes this mathematically possibe (check link above)\n",
    "\n",
    "2) An intuitive reason : if all events happen with the same probability p (such as a fair coin or a fair die - uniform distribution), the number of events is 1/p. For capturing 1/p events in binary notation, we have to use $log_{2}(1/p)$ bits\n",
    "This extends even in a non-uniform distribution because of theoretical reasons (see link above)\n",
    "\n",
    "\n",
    "What base of the logarithm should we take in the definition above ?\n",
    "\n",
    "Depends - base 2 gives an output as bits, base e as nats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a fair coin, with probabilty of each event = 1/2,  what is information of the event ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information of event E in a fair coin toss is 1.0 bits\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "information = log2(1/(1/2))\n",
    "print(\"information of event E in a fair coin toss is {} bits\".format(information))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a fair die, with probabilty of each event = 1/6,  what is information of the event ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information of event E in a fair die roll is 2.584962500721156 bits\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "information = log2(1/(1/6))\n",
    "print(\"information of event E in a fair die roll is {} bits\".format(information))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, we see that since a p(E) for a die is 1/6 (lower than p(E) for a coin),\n",
    "information of event in a die roll is higher than coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy in information theory is analogous to the concept of entropy in thermodynamic systems\n",
    "\n",
    "Specifically, the entropy of a random variable X is the [average level of \"information\", \"surprise\", or \"uncertainty\" inherent in the variable's outcomes](https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "Note that entropy is defined for a random variable, whereas information is defined for an event\n",
    "\n",
    "Entropy is nothing but the expectation of information\n",
    "\n",
    "ie H(X) = E(I(X)) =  $\\sum_{i}p(Ei)log(\\frac{1}{p(E(i))})$\n",
    "\n",
    "This is the equation of Shannon Entropy, over events/states of the RV . This is analogous to the equation of Gibbs entropy in thermodynamics over states of the thermodynamical system\n",
    "\n",
    "\n",
    "Note that by the definition above, entropy is always positive. The lowest value it can take is 0, for a certain event. For example, a RV X for a coin toss where p(X=H) = 1, p(X=T) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you compare entropy of tossing a fair coin vs a biased coin where p(H) = 0.7 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def entropy(prob_vec):\n",
    "    prob_vec = np.array(prob_vec)\n",
    "    prob_vec = prob_vec[prob_vec>0]\n",
    "    return np.sum(-prob_vec*np.log2(prob_vec))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of a fair coin is 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of a fair coin is {} bits\".format(entropy([0.5, 0.5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of a biased coin with p(H) = 0.7  is 0.8812908992306927\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of a biased coin with p(H) = 0.7  is {} bits\".format(entropy([0.7, 0.3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that entropy of fair coin is larger, in fact , a uniform distribution has the highest entropy among all distributions, because there is more uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, assume a bernoulli distribution X ~ Bern(p)\n",
    "where p(X=1) = p, and p(X=0) = 1-p\n",
    "\n",
    "Therefore, H(X) = -plogp - (1-p)log(1-p)\n",
    "\n",
    "Parametrized by p, we can call this H(p) = -plogp - (1-p)log(1-p)\n",
    "\n",
    "This  function H(p) is called binary entropy function\n",
    "\n",
    "If we plot, this is a concave function, whose maximum occurs when p = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![equation2](bernoulli_plot.png \"Figure 2.1 Elements of Information theory\") \n",
    "\n",
    "Image credit - Figure 2.1 Elements of Information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof that H(p) is  a concave function of p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the same domain D, assume there are two RV X with PMF p, and Y with pmf q\n",
    "\n",
    "Define another RV B which can take two values, 0 and 1 with the following PMF\n",
    "\n",
    "P(B=0) = $\\lambda$, P(B=1) = 1 - $\\lambda$\n",
    "\n",
    "Define a new RV Z such that Z=X if B=0, Z=Y if B=1\n",
    "\n",
    "Therefore, The distribution of Z is $\\lambda*p + (1-\\lambda)*q $\n",
    "\n",
    "We know that conditioning always reduces entropy\n",
    "\n",
    "Therefore , H(Z) >= H(Z|B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(Z|B) = $P(B=0)* H(Z|B=0) + P(B=1)*H(Z|B=1)$ by definition\n",
    "       = $\\lambda*H(X) + (1-\\lambda)*H(Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(Z) = $H(\\lambda*X + (1-\\lambda)*Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since H(Z) >= H(Z|B),\n",
    "\n",
    "$H(\\lambda*X + (1-\\lambda)*Y)$ >= $\\lambda*H(X) + (1-\\lambda)*H(Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proves that H(X) is concave by Jensen's inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 <= H(X) <= log(L) where L is the number of possible values X can take\n",
    "\n",
    "H(X) = 0 only if P(X=x) = 1 for some X (completely predictable)\n",
    "\n",
    "H(X) = log(L) only if P(X=x) = (1/L) for all X (uniform distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof of  \n",
    "\n",
    "H(X) >= 0\n",
    "\n",
    "\n",
    "H(X) = $-P(X=x)log(P(X=x))$\n",
    "\n",
    "when P(X=x) =1 for some x,  -P(X=x)log(P(X=x) = 0\n",
    "\n",
    "when P(X=x) < 1 for some x, log(P)<0 =>    -P(X=x)log(P(X=x)  >0\n",
    "\n",
    "Therefore, H(X) = 0 iff P(X=x)=1 for some x in X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof of H(X) <= Log(L)\n",
    "\n",
    "Use IT inequality log(r) <= (r-1)log(e) if r > 0\n",
    "\n",
    "Take H(X)-log(L)\n",
    "\n",
    "= -($\\sum_{X}P(X=x)log(P(X=x))$) - log(L)\n",
    "\n",
    "= -($\\sum_{X}P(X=x)log(P(X=x))$) - $\\sum_{X}P(X=x)log(L)$\n",
    "\n",
    "\n",
    "= $\\sum_{X}P(X=x)(log(\\frac{1}{L*P(X=x)}))$\n",
    "\n",
    "<= $\\sum_{X}P(X=x)((\\frac{1}{L*P(X=x)} - 1)*log(e))$\n",
    "\n",
    "<= log(e)*($\\sum_{X=x}\\frac{1}{L}$ - $\\sum_{X=x}P(X=x)$)\n",
    "\n",
    "<= log(e)*(1-1) <= 0\n",
    "\n",
    "\n",
    "Therefore, H(X) <= log(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending to continuous RV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical definition of entropy according to Shannon as defined above was postulated for only discrete RV\n",
    "\n",
    "Shannon tried to extend the concept of entropy to continuous RV is called a differential entropy, which has some noticeably different properties from classical entropy (thus , it is not theoretically the best estimate of uncertainty for continuous RV . In fact , it can even be negative, and is not invariant to transformations). [See here for more details](https://en.wikipedia.org/wiki/Differential_entropy)\n",
    "\n",
    "This has been found to be \n",
    "\n",
    "First the definition\n",
    "\n",
    "Given a continuous RV X\n",
    "\n",
    "the differential entropy h(X) = E(-ln(f(x))) = -$\\int_{X}f(x)ln(f(x))dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example where differential entropy can be negative\n",
    "\n",
    "Assume X ~ U(0,0.5) =>\n",
    "\n",
    "f(x) = 2\n",
    "\n",
    "Therefore, h(X) =  -$\\int_{0,2}*2*ln(2)dx$\n",
    "\n",
    "= -0.5*ln(2)*2 which is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional entropy of RV Y|X  (represented as H(Y|X)) is the amount of information (think of it as bits) needed to describe Y given another RV X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that H(Y) = E(I(Y)) =  $\\sum_{i}p(Ei)log(\\frac{1}{p(E(i))})$ over all Events Ei in Y\n",
    "\n",
    "which can be equivalently written as $\\sum_{y}p(Y=y)log(\\frac{1}{p(Y=y)})$\n",
    "over all outcomes y of RV Y\n",
    "in a discete case\n",
    "\n",
    "\n",
    "Assume that RV X takes a specific value x\n",
    "\n",
    "Then, we can say as an extension that \n",
    "H(Y|X=x) = $\\sum_{y}p(Y=y|X=x)log(\\frac{1}{p(Y=y|X=x)})$\n",
    "\n",
    "By definition, conditional entropy H(Y|X) is defined as weightedsum of H(Y|X=x) over all values of X. Thus, taking a weighted averaging over all possible values of X,\n",
    "we can define\n",
    "\n",
    "H(Y|X) = $\\sum_{x}p(X=x)H(Y|X=x)$ \n",
    " = $\\sum_{x}p(X=x) (\\sum_{y}p(Y=y|X=x)log(\\frac{1}{p(Y=y|X=x)}))$ \n",
    "  = $\\sum_{x}\\sum_{y}p(X=x,Y=y)log(\\frac{1}{p(Y=y|X=x)}))$ \n",
    "  \n",
    "  = $\\sum_{x,y}p(X=x,Y=y) log(\\frac{p(X=x)}{p(X=x, Y=y)})) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of conditional entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property 1 - If X and Y are independent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X and Y are independent RV, intuitively, H(Y|X) = H(Y)\n",
    "\n",
    "This can also be seen from the equation above. If X and Y are independent,\n",
    "p(X=x, Y=y) = p(X)p(Y)\n",
    "\n",
    "H(Y|X) = $\\sum_{x,y}p(X=x)*p(Y=y) log(\\frac{p(X=x)}{p(X=x)*p( Y=y)})) $\n",
    "\n",
    "= $\\sum_{x,y}p(X=x)*p(Y=y)log(\\frac{1}{p(Y=y)})$\n",
    "\n",
    "= $\\sum_{y}p(Y=y)log(\\frac{1}{p(Y=y)})$ = H(Y)\n",
    "\n",
    "\n",
    "This also implies that H(X,Y) = H(X) + H(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property 2 - when is conditional entropy 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If H(Y|X)=0, it means Y is completely dependent on X / completely determined by X (ie P(X,Y) = P(X)), which can also be seen from the equation above\n",
    "\n",
    "Note, just like entropy for discrete variables, conditional entropy is also always positive, with a minimum value of 0 if rv are completely related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property 3 (Chain Rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(Y|X) = H(Y,X) - H(X)\n",
    "\n",
    "Trivial from equation above \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property 4 - generalization of chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, H(X1,X2...Xn) = H(X1) + H(X2|X1) + H(X3|X1,X2).... (generalizing equation above)\n",
    "\n",
    "Kind of analogous to chain rule of probability of random variables, except that is multiplicative instead of additive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property 5 - Bayes rule of entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(Y|X) = H(X|Y) + H(Y) - H(X)\n",
    "\n",
    "Proof : \n",
    "H(Y|X) = H(Y,X) - H(X) = H(X,Y) - H(Y)\n",
    "H(Y|X) + H(X) = H(X|Y) + H(Y) = H(X,Y)\n",
    "=> H(Y|X) = H(X|Y) + H(Y) - H(X)\n",
    "\n",
    "Again, kind of analogous to Bayes theorem of probability , except that is additive unlike probability which is multiplicative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property 6 - H(Y|X) <= H(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof : \n",
    "\n",
    "Define relative entropy I(Y|X) = H(Y)-H(Y|X)\n",
    "\n",
    "Prove that I(Y|X) >= 0\n",
    "\n",
    "\n",
    "H(Y) =  -$\\sum_{y}p(Y=y)log({p( Y=y)})) $   \n",
    "H(Y|X) = $\\sum_{x,y}p(X=x,Y=y) log(\\frac{p(X=x)}{p(X=x, Y=y)})) $\n",
    "\n",
    "\n",
    "\n",
    "I =  -$\\sum_{y}p(Y=y)log({p( Y=y)})) $    - $\\sum_{x,y}p(X=x,Y=y) log(\\frac{p(X=x)}{p(X=x, Y=y)})) $\n",
    "\n",
    "\n",
    "\n",
    "$\\sum_{y}p(Y=y)log({p( Y=y)})) $ =  $\\sum_{x,y}p(X=x, Y=y)log({p( Y=y)})) $\n",
    "\n",
    "\n",
    "Therefore, I  = -  $\\sum_{x,y}p(X=x, Y=y)log({p( Y=y)})) $ - $\\sum_{x,y}p(X=x,Y=y) log(\\frac{p(X=x)}{p(X=x, Y=y)})) $\n",
    "\n",
    "\n",
    "= - ($\\sum_{x,y} (p(X=x, Y=y)log({p( Y=y)}) + p(X=x,Y=y) log(\\frac{p(X=x)}{p(X=x, Y=y)})))$)\n",
    "\n",
    "= $\\sum_{x,y} (p(X=x, Y=y)*log(\\frac{p(X=x,Y=y)}{p(X=x)*p(Y=y)})))$\n",
    "\n",
    "\n",
    "Multiplying and dividing numerator and denominator by p(X=x)*p(Y=y), we get\n",
    "\n",
    "\n",
    "I = $\\sum_{x,y} p(X=x)*p(Y=y)  * \\frac{(p(X=x, Y=y)}{p(X)*p(Y)}*log(\\frac{p(X=x,Y=y)}{p(X=x)*p(Y=y)})))$\n",
    "\n",
    "\n",
    "\n",
    "By Jensen's inequality, for convex functions, E(f(X)) >= f(E(X))\n",
    "\n",
    "xlogx is a convex function for x > 0\n",
    "\n",
    "Therefore, I =  $\\sum_{x,y} p(X=x)*p(Y=y)  *  f(\\frac{(p(X=x, Y=y)}{p(X)*p(Y)})$\n",
    "\n",
    "where f(z) = zlogz\n",
    "\n",
    "\n",
    ">=  $  f (\\sum_{x,y} p(X=x)*p(Y=y)  *  \\frac{(p(X=x, Y=y)}{p(X)*p(Y)})) $ \n",
    "\n",
    ">= f(1) >=0\n",
    "\n",
    "Hence Proved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending to continuous RV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h(Y|X) is called differential conditional entropy in the continuous case, defined as\n",
    "\n",
    "\n",
    "h(Y|X) =  = -$\\int_{X,Y}f(x,y)ln(f(y|x))dxdy$\n",
    "\n",
    "Just as in the discrete case, h(Y|X) = h(X,Y) - h(Y)\n",
    "\n",
    "However, if any of the entropies don't exist, this equation does not hold\n",
    "\n",
    "Also, h(Y|X) <= h(Y), with equality if X and Y are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure of entropy asscoiated with a set of RV jointly\n",
    "\n",
    "For example, H(X,Y) is used to represent the joint entropy of X and Y\n",
    "\n",
    "Defined as H(X,Y) = -$\\sum_{x,y}p(X=x, Y=y)log(p(X=x, Y=y))$ represented \n",
    "in convenient notation as H(X,Y) = -$\\sum_{x E X, y E Y}p(x,y)log(p(x,y))$\n",
    "\n",
    "\n",
    "Can be extended to N Variables X1..XN\n",
    "\n",
    "as H(X1...Xn) =  -$\\sum_{x1 E X1, x2 E X2.. x3 E Xn}p(x1..xN)log(p(x1...xN))$\n",
    "\n",
    "\n",
    "We've already seen this above in the definiton of conditional entropy,\n",
    "\n",
    "where H(Y|X) = H(X,Y) - H(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of joint entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property 1 \n",
    "\n",
    "Just like other entropy measurements, joint entropy is non negative. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property 2 -\n",
    "\n",
    "The joint entropy of a set of RV is greater than or equal to the maximum of the entropies of the individual RV\n",
    "\n",
    "H(X1,X2..Xn) >= max(H(X1), H(X2)...)\n",
    "\n",
    "Start with two variables\n",
    "\n",
    "H(X,Y) = H(X) + H(Y|X) - from definitions earlier\n",
    "\n",
    "We know H(Y) >= 0\n",
    "\n",
    "=> H(X,Y) >= H(X)\n",
    "\n",
    "Similarly, H(X,Y) >= H(Y)\n",
    "\n",
    "This implies that H(X,Y) >= max(H(X), H(Y))\n",
    "\n",
    "Can extend to entropy of more than two RV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property 3 - \n",
    "\n",
    "joint entropy is less than or equal to sum of individual entropies. In fact, equality occurs when All\n",
    "RV are independent\n",
    "\n",
    "H(X1,X2..XN) <= $\\sum_{i}H(Xi)$\n",
    "\n",
    "Starting with two RV again\n",
    "\n",
    "(Proven in section below on Information gain using Jenson's inequality)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![equation3](simple_problem.png \"Example 2.2.1 Elements of Information theory\") \n",
    "\n",
    "Image credit - Example 2.2.1 Elements of Information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given Joint distribution above, compute joint and conditional entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.125   0.0625  0.03125 0.03125]\n",
      " [0.0625  0.125   0.03125 0.03125]\n",
      " [0.0625  0.0625  0.0625  0.0625 ]\n",
      " [0.25    0.      0.      0.     ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.zeros(shape = (4,4))\n",
    "a[0,0] = 1/8\n",
    "a[0,1] = 1/16\n",
    "a[0,2] = 1/32\n",
    "a[0,3] = 1/32\n",
    "\n",
    "a[1,0] = 1/16\n",
    "a[1,1] = 1/8\n",
    "a[1,2] = 1/32\n",
    "a[1,3] = 1/32\n",
    "\n",
    "a[2,0] = 1/16\n",
    "a[2,1] = 1/16\n",
    "a[2,2] = 1/16\n",
    "a[2,3] = 1/16\n",
    "\n",
    "a[3,0] = 1/4\n",
    "a[3,1] = 0\n",
    "a[3,2] = 0\n",
    "a[3,3] = 0\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint entropy H(X,Y) is 3.375\n"
     ]
    }
   ],
   "source": [
    "hxy = (entropy(a.flatten().tolist()))\n",
    "print(\"joint entropy H(X,Y) is {}\".format(hxy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy H(X) is 1.75\n"
     ]
    }
   ],
   "source": [
    "px = a.sum(axis=0)\n",
    "hx = entropy(px)\n",
    "\n",
    "print(\"entropy H(X) is {}\".format(hx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy H(Y) is 2.0\n"
     ]
    }
   ],
   "source": [
    "py = a.sum(axis=1)\n",
    "hy = entropy(py)\n",
    "\n",
    "print(\"entropy H(Y) is {}\".format(hy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(X|Y) = $\\sum_{Y=i}P(Y=i)H(X|Y=i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditional entropy H(X|Y) is 1.375\n"
     ]
    }
   ],
   "source": [
    "hxgiveny = 0\n",
    "for i in range(len(py)):\n",
    "    pxgivenyequalsi = a[i,]/py[i]\n",
    "    hxgiveny = hxgiveny + py[i]*entropy(pxgivenyequalsi.tolist())\n",
    "print(\"conditional entropy H(X|Y) is {}\".format(hxgiveny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditional entropy H(Y|X) is 1.625\n"
     ]
    }
   ],
   "source": [
    "hygivenx = 0\n",
    "for i in range(len(px)):\n",
    "    pygivenxequalsi = a[:,i]/px[i]\n",
    "    hygivenx = hygivenx + px[i]*entropy(pygivenxequalsi.tolist())\n",
    "print(\"conditional entropy H(Y|X) is {}\".format(pygivenx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As derived earlier, we note that H(X) + H(Y|X) 1.75 + 1.625 = 3.375 is same as H(X,Y) 3.375\n"
     ]
    }
   ],
   "source": [
    "print(\"As derived earlier, we note that H(X) + H(Y|X) {0} + {1} = {2} is same as H(X,Y) {3}\".format(hx, hygivenx, hx + hygivenx,  hxy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarly, we note that H(Y) + H(X|Y) 2.0 + 1.375 = 3.375 is same as H(X,Y) 3.375\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarly, we note that H(Y) + H(X|Y) {0} + {1} = {2} is same as H(X,Y) {3}\".format(hy, hxgiveny, hy + hxgiveny, entropy(a.flatten().tolist()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information I = H(X) + H(Y) - H(X,Y) = 1.75 + 2.0 - 3.375 = 0.375 bits\n"
     ]
    }
   ],
   "source": [
    "print(\"Mutual Information I = H(X) + H(Y) - H(X,Y) = {0} + {1} - {2} = {3} bits\".format(hx, hy, hxy, hx+hy-hxy ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELATIVE ENTROPY OR KL DIVERGENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative entropy or KL divergence is a way to quantify distance between two dstributions\n",
    "\n",
    "If X and Xhat are two RV, having pmf p and q respectively,\n",
    "\n",
    "D(p||q) is the distance between distributions with pmf p and q \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula - \n",
    "\n",
    "D(p||q) = $\\sum_{xEX}p(X=x)log(\\frac{p(X=x)}{q(X=x)})$ = -$\\sum_{xEX}p(X=x)log(\\frac{q(X=x)}{p(X=x)})$\n",
    "\n",
    "\n",
    "In words, it is the expectation of the log difference between pmf p and q, expectation taken over p,\n",
    "where p and q have to be defined over the same probability space X\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem : assume Xhat is a uniform distribution , with q(Xhat=x) = 1/L\n",
    "\n",
    "and X is the real distribution\n",
    "\n",
    "Then, D(p||q) = -H(X) + log(L) = log(L) - H(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that relative entropy is defined only if p(X)=0 for any X=x implies that q(X)=0 for same x, as otherwise we get log(0) which is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension to continuous RV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D(p||q) = $\\int_{-inf, +inf}p(x)log(\\frac{p(x)}{q(x)})$ =  -$\\int_{-inf, +inf}p(x)log(\\frac{q(x)}{p(x)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume P(X)is a binomial distribution with N=2, p=0.4. therefore\n",
    "P(X=0) = 9/25, P(X=1) = 12/25, P(X=2) = 4/25\n",
    "\n",
    "Let Q(Y) be a uniform distribution on the same domain\n",
    "Q(Y=0) = 1/3, Q(Y=1) = 1/3, Q(Y=2) = 1/3\n",
    "\n",
    "We want to evaluate D(P||Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def relative_entropy(p, q):\n",
    "    rel_env = None\n",
    "    if len(p) != len(q):\n",
    "        print(\"p and q have different lengths, returning\")\n",
    "    else:\n",
    "        try:\n",
    "            rel_env = np.sum([p[i]*np.log(p[i]/q[i]) for i in range(len(p))])\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    return rel_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [9/25, 12/25, 4/25]\n",
    "q = [1/3, 1/3, 1/3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0852996013183706\n"
     ]
    }
   ],
   "source": [
    "print(relative_entropy(p,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09745500678538754\n"
     ]
    }
   ],
   "source": [
    "print(relative_entropy(q,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note from above that D(p||q) != D(q|p)\n",
    "Also, relative entropy does not follow the triangle inequality, so its not a true [metric](https://en.wikipedia.org/wiki/Metric_(mathematics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretations of relative entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) In ML, such as in decision trees, D(p||q) is called [information gain](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees) achieved if p were used instead of q   \n",
    "2) In a Bayesian interpretation, it is the amount of information lost when Q (usually prediction) is used to approximate P (the true distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional relative entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two joint mass functions p(x,y) and q(x,y), conditional relative entropy D(p(y|x) || q(y|x)) is defined as the relative entropy between p(y|x) and q(y|x) averaged over p(x)\n",
    "\n",
    "\n",
    "In equation , D(p(y|x) || q(y|x)) = \n",
    "\n",
    "$\\sum_{x}p(x)(\\sum_{y}p(y|x)log(\\frac{p(y|x)}{q(y|x)}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of relative entropy/information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) D(p||q) is always >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof 1 : \n",
    "    \n",
    "D(p||q) =   -$\\sum_{xEX}p(X=x)log(\\frac{q(X=x)}{p(X=x)})$ =>\n",
    "\n",
    "-D(p||q) = $\\sum_{xEX}p(X=x)log(\\frac{q(X=x)}{p(X=x)})$ \n",
    "\n",
    "By Jensen's inequality, log is a concave function => E(log X) <= log(E(X))\n",
    "\n",
    "Therefore , $\\sum_{xEX}p(X=x)log(\\frac{q(X=x)}{p(X=x)})$  <= log($\\sum_{xEX}q(X=x)$) <=0\n",
    "\n",
    "-D(p||q) <= 0 => D(p||q) >=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof 2 :  Using IT inequality\n",
    "\n",
    "-D(p||q) = $\\sum_{xEX}p(X=x)log(\\frac{q(X=x)}{p(X=x)})$ \n",
    "\n",
    "By IT inequality, log(r) <= (r-1)*log(e)\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$\\sum_{xEX}p(X=x)log(\\frac{q(X=x)}{p(X=x)})$ \n",
    "\n",
    "<= $\\sum_{xEX}p(X=x)(\\frac{q(X=x)}{p(X=x)} - 1)*log(e))$\n",
    "\n",
    "<= 0 \n",
    "\n",
    "Hence proved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Relative entropy is additive for independent distributions   \n",
    "\n",
    "ie If P(x,y) = P(x)P(y)  \n",
    "\n",
    "If Q(x,y) = Q(x)Q(y)  \n",
    "\n",
    "\n",
    "D(P||Q) = D(P1||Q1) + D(P2||Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) D(p||q) is convex in the pair p,q\n",
    "\n",
    "ie If p1,q1 and p2,q2 are two pairs of probability mass functions,\n",
    "\n",
    "$D(\\frac{\\lambda*p1 + (1-\\lambda)*p2}{\\lambda*q1 + (1-\\lambda)*q2})$ <=\n",
    "\n",
    "$\\lambda*D(\\frac{p1}{q1}) + (1-\\lambda)*D(\\frac{p2}{q2}))$\n",
    "\n",
    "for all $0 <= \\lambda <= 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof :\n",
    "\n",
    "LHS :\n",
    "\n",
    "$D(\\frac{\\lambda*p1 + (1-\\lambda)*p2}{\\lambda*q1 + (1-\\lambda)*q2})$\n",
    "\n",
    "\n",
    "= $\\sum_{X}(\\lambda*p1 + (1-\\lambda)*p2)log(\\frac{\\lambda*p1 + (1-\\lambda)*p2}{\\lambda*q1 + (1-\\lambda)*q2})$\n",
    "\n",
    "by definition of relative entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log sum inequality says that\n",
    "\n",
    "For non negative numbers a1.. an and b1..bn, \n",
    "\n",
    "$\\sum_{i=1:n}(ai*log (\\frac{ai}{bi}))$ >= $(\\sum_{i=1:n}(ai))*log(\\frac{\\sum_{i=1:n}(ai)}{\\sum_{i=1:n}(bi)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Therefore, by log sum inequality,\n",
    "\n",
    "\n",
    "$\\sum_{X}(\\lambda*p1 + (1-\\lambda)*p2)log(\\frac{\\lambda*p1 + (1-\\lambda)*p2}{\\lambda*q1 + (1-\\lambda)*q2})$\n",
    " can be taken as the RHS of the log sum inequality\n",
    " \n",
    " where $a1 = \\lambda*p1$, $a2 = (1-\\lambda)*p2$,\n",
    " $b1 =  \\lambda*q1$, $b2 = (1-\\lambda)*q2$\n",
    "\n",
    "Therefore, RHS <=\n",
    " \n",
    " $\\sum_{X} (\\lambda*p1*log(\\frac{\\lambda*p1}{\\lambda*q1})  +  ((1-\\lambda)*p2*log(\\frac{(1-\\lambda)*p2}{(1-\\lambda)*q2})  )$\n",
    " \n",
    " = $\\lambda*D(\\frac{p1}{q1}) + (1-\\lambda)*D(\\frac{p2}{q2})$\n",
    " \n",
    " \n",
    " Hence proved\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information of two RV is a measure of mutual dependence between two RV . It quantifies amount of information (in units such as bits or nats) obtained about one RV by observing the other RV\n",
    "\n",
    "It is defined as I(X;Y) =  D(P(X,Y) || P(X)P(Y))\n",
    "\n",
    "(the relative entropy between the joint probability and product of marginal probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I(X;Y) =  D(P(X,Y) || P(X)P(Y)) = $\\sum_{X,Y}P(X,Y)log(\\frac{P(X,Y)}{P(X)P(Y)})$ in a disrete case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a continuous case,\n",
    "\n",
    "I(X;Y) = $\\int_{X,Y}p(X,Y)log(\\frac{p(X,Y)}{p(X)p(Y)})dxdy$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, it talks about Information X and Y share. \n",
    "\n",
    "If I(X;Y) = 0, it means X and Y are completely independent\n",
    "\n",
    "On the other hand, If X and Y are completely dependent,\n",
    "\n",
    "I(X;Y) = H(X) = H(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between mutual information and entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the discrete case, I(X;Y) =  D(P(X,Y) || P(X)P(Y)) = $\\sum_{X,Y}P(X,Y)log(\\frac{P(X,Y)}{P(X)P(Y)})$ \n",
    "\n",
    "\n",
    "= $\\sum_{X,Y}P(X,Y)log(\\frac{P(X,Y)}{P(X)}) - \\sum_{X,Y}P(X,Y)log(P(Y))$  =  H(Y) - H(Y|X)\n",
    "\n",
    "\n",
    "= $\\sum_{X,Y}P(X,Y)log(\\frac{P(X,Y)}{P(Y)}) - \\sum_{X,Y}P(X,Y)log(P(Y))$  =  H(X) - H(X|Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also know that H(Y|X) = H(X,Y) - H(X)\n",
    "\n",
    "Therefore , I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "\n",
    "\n",
    "Note that this also can be written as \n",
    "\n",
    "\n",
    "I(X;Y) = H(X) - H(X|Y)\n",
    "\n",
    "or equivalently\n",
    "\n",
    "I(X;Y) = H(Y) - H(Y|X)\n",
    "\n",
    "\n",
    "In case X and Y are completely independent , I(X,Y) = 0\n",
    "\n",
    "See diagram below for more clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![equation1](mi_entropy_relation.png \"Figure 2.2 Elements of Information theory\") \n",
    "\n",
    "Image credit - Figure 2.2 Elements of Information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Mutual Information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I(X;Y|Z) = I(X|Z) - I(X|Y,Z) = I(Y|Z) - I(Y|X,Z)\n",
    "\n",
    "This is defined as $E_{p(x,y,z)}log\\frac{p(X,Y|Z)}{p(X|Z)*p(Y|Z)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I(X;Y) >= 0   => Since I(X;Y) is a kind of relative entropy, and we have already proven that relative entropy >=0 always, I(X;Y) >= 0\n",
    "\n",
    "\n",
    "This also implies that H(X) + H(Y) >= H(X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. I(X;X) = H(X) + H(X) - H(X,X) = H(X)\n",
    "\n",
    "So information gain of a RV with itself is just the entropy of the RV itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Chain rule for Mutual Information\n",
    "\n",
    "I(X1,X2..Xn;Y) = $\\sum_{i=1}^{n}I(Xi;Y|X1..Xi-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof :\n",
    "\n",
    "I(X1,X2..Xn;Y) = H(X1,X2..Xn) - H(X1,...Xn|Y) = \n",
    "\n",
    "$\\sum_{i=1}^{n}H(Xi|X1..Xi-1)$ - $\\sum_{i=1}^{n}H(Xi|X1..Xi-1,Y)$\n",
    "\n",
    "\n",
    "=$\\sum_{i=1}^{n}(I(Xi;Y|X1...Xi-1))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Mutual information I(X;Y) is a concave function of p(x) for fixed p(y|x), and a convex function of p(y|x) for a fixed p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proof (using explicit notation to make it less confusing)\n",
    "\n",
    "a) $p_{Y}(Y=y) = \\sum_{X=x}(p_{X}(X=x)p_{X,Y}(\\frac{Y=y}{X=x}))$\n",
    "\n",
    "If $p_{X,Y}(\\frac{Y=y}{X=x})$ is fixed for every X, it means $p_{Y}(Y=y)$ is a linear function of $p_{X}(X=x)$\n",
    "\n",
    "\n",
    "We know that I(X;Y) = H(Y) - H(Y|X) = H(Y) - $\\sum_{x}P(X=x)H(\\frac{Y}{X=x})$\n",
    "\n",
    "\n",
    "H(Y) is a concave function of p(Y=y), but since p(Y=y) is a linear function of p(X=x), it is a concave function of p(X)\n",
    "\n",
    "The second term is a linear function in p(X), which is again concave\n",
    "\n",
    "Therefore I(X;Y) is concave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem on mutual information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you have a fair die and a fair coin\n",
    "You toss the die first, if die has outcome in (1,2,3,4), you toss the coin once\n",
    "\n",
    "If die has outcome in (5,6), you toss the coin twice\n",
    "\n",
    "Find mutual information between face of die, and no of heads obtained\n",
    "\n",
    "Let X be RV describing die, Y be RV counting no of heads\n",
    "\n",
    "P(X in (1,2,3,4)) = 2/3\n",
    "P(X in (5,6)) = 1/3\n",
    "\n",
    "P(Y=0) = P(Y=0|X in (1,2,3,4))*P(X in (1,2,3,4)) + \n",
    "P(Y=0|X in (5,6))*P(X in (5,6))\n",
    "\n",
    "= (1/2)*(2/3)  + (1/4)*(1/3) = (5/12)\n",
    "\n",
    "\n",
    "P(Y=1) = P(Y=1|X in (1,2,3,4))*P(X in (1,2,3,4)) + \n",
    "P(Y=1|X in (5,6))*P(X in (5,6))\n",
    "\n",
    "= (1/2)*(2/3)  + (1/2)*(1/3) = (1/2)\n",
    "\n",
    "\n",
    "P(Y=2) = P(Y=2|X in (1,2,3,4))*P(X in (1,2,3,4)) + \n",
    "P(Y=2|X in (5,6))*P(X in (5,6))\n",
    "\n",
    "= 0*(2/3)  + (1/4)*(1/3) = (1/12)\n",
    "\n",
    "H(Y) = H(5/12, 1/2, 1/12) = 1.325\n",
    "\n",
    "H(Y|X) = P(X in (1,2,3,4))*H(Y|X in (1,2,3,4)\n",
    "\n",
    "+ P(X in (5,6))*H(Y|X in (5,6)\n",
    "\n",
    "\n",
    "= (2/3)*H(1/2, 1/2,0 ) + (1/3)*H(1/4, 1/2, 1/4) = 1.167\n",
    "\n",
    "I(X;Y) = H(Y)-H(Y|X) = 1.325-1.167 = 0.158"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) https://machinelearningmastery.com/what-is-information-entropy/  \n",
    "2) https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "3) https://stats.stackexchange.com/questions/87182/what-is-the-role-of-the-logarithm-in-shannons-entropy#:~:text=We%20can%20call%20log(1,events%20we%20can%20tell%20apart).    \n",
    "4) https://machinelearningmastery.com/information-gain-and-mutual-information/\n",
    "    \n",
    "5) Elements of Information Theory (Cover and Thomas) Chapter 2\n",
    "6) https://www.youtube.com/watch?v=JxNirQxjvs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
