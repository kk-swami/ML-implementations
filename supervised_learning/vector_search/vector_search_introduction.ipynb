{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e2d500-3103-4bf0-a14f-bb4550429810",
   "metadata": {},
   "source": [
    "Searching for a query in embedding form (for example sentence transformers) against an embeddding database is an O(N) operation for each query\n",
    "\n",
    "So if you have M queries and N is the size of your corpus embedding, we are talking about an O(M * N) operation which is very expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5d97b-176f-4e81-8b63-67d3f348a6ac",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e456b7-d601-4f7d-b9e6-56096dcc403b",
   "metadata": {},
   "source": [
    "1) Given an image of a building, in your corpus, find all other images of this building\n",
    "This requires you to construct a numeric representation (embedding) of all images in your corpus, and find the vectors which are closest to your query image (the building image)\n",
    "\n",
    "Closest can be defined as vectors which are similar by euclidean distance\n",
    "\n",
    "2) Maximum inner product search\n",
    "\n",
    "Let's say you have a query (can be a user in a recommendation system context ) which can be represented as an embedding of dimension N. If we want to find out which items the user is likely to buy, (assuming the user-item embedding has been trained jointly) , you want to find out which items in our corpus the query has the largest dot product with\n",
    "\n",
    "For both these above problems (given a query, find vectors in database which are closest from a euclidean distance standpoint; and given a query, find vectors in database which have the highest dot product), we need to do this at scale (since your database can have billions of entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f22dd2-2065-42d1-9055-3ade5cebf352",
   "metadata": {},
   "source": [
    "Faster algorithms include ANNOY from Spotify, FAISS from facebook, hnswlib\n",
    "The other sections in this folder will talk about those implementations and how they speed up this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a6c8c-46e0-4e9e-b574-56d3c6e91575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "525e8960-f7c4-40f6-bbde-f8a96c26a394",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb286b-af1a-4205-bb91-1d7c4e07b055",
   "metadata": {},
   "source": [
    "https://levelup.gitconnected.com/vector-database-the-secret-behind-large-language-models-capabilities-7d4f6b714d16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0d410-baf3-49c1-9be8-d966e452d014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
