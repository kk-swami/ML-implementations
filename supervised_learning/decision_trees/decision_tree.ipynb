{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are a class of supervised ML algorithms used for both classification and regression . There are several variants of decision trees, of which CART (classification and regression trees) is the most popular . Other variants are ID3 and C4.5\n",
    "\n",
    "Advantages - 1) Easy to interpret (almost think of it as a bunch of if else)\n",
    "Disadvantages - Prone to overfit, for which Bagging/Boosting methods are a popular solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work ? Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of it like a tree based algorithm, where at every node, a decision using a set of feature is made. \n",
    "\n",
    "During training, the model using gini gain or entropy gain, which feature to use at every single node are decided\n",
    "\n",
    "CART for example uses Gini gain, where ID3 and C4.5 use entropy gain\n",
    "\n",
    "\n",
    "The goal is to keep each split as \"pure\" as possible to achieve classification. For example, in theory, if there are 4 classes, we would like 4 leaf nodes, each leaf node capturing all the sample points of that class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait a minute, what are gini gain and entropy gain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini gain and entropy gain are two alternative cost functions used for decision trees. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini index\n",
    "\n",
    "The formula for Gini index is   1 -  $\\sum_{i}{p_{i}}^{2}$ or equivalently,\n",
    "$\\sum_{i}p_i*({1-p_{i}})$ which are the same\n",
    "\n",
    "where the sum is across estimated probabilities of all classes within each group.\n",
    "Then the gini index for all groups are summed together to get overall gini index for the split\n",
    "\n",
    "One estimate of how effective the split was at a node\n",
    "For example, at a node, if based on a condition learnt at a node, the data is split into two groups\n",
    "\n",
    "Say all points with Feature X1 < 5 goes to one node (group G1), and all with X1 >= 5 goes to another node (group G2). Assume only two classes in GT C1 and C2\n",
    "\n",
    "We get a perfect split if G1 and G2 belong to completely different classes\n",
    "\n",
    "## How do we mathematically formulate this ?\n",
    "\n",
    "If pC1G1 is the estimated fraction of points in group G1 belong to class C1 (just the count of all points in G1 belonging to C1 divided by total no of samples in G1) and pC2G1 is estimated fraction of points in group G1 \n",
    "\n",
    "gini index of Group G1  GiniG1= 1 - $({pC1G1}^{2} + {pC2G1}^{2})$\n",
    "\n",
    "Similarly , gini index of Group G2 GiniG2 = 1 - $({pC1G2}^{2} + {pC2G2}^{2})$\n",
    "\n",
    "\n",
    "Overall Gini index = GiniG1 + GiniG2 \n",
    "\n",
    "## So what happens if split is completely perfect or completely imperfect ?\n",
    "\n",
    "For a completely perfect split, pC1G1 = 1, pC2G2=1, pC1G2 = 0, pC2G1 = 0\n",
    "\n",
    "Therefore, GiniG1 = 1 - (1 + 0) = 0\n",
    "GiniG2 = 0\n",
    "\n",
    "Theferefore, Gini Index for a perfect split is 0\n",
    "\n",
    "\n",
    "For a completely imperfect split, all 4 probabilities will be 0.5\n",
    "\n",
    "GiniG1 = (1 - $({0.5}^{2} + {0.5}^{2})$ = 1 - 0.5 = 0.5\n",
    "Similarly, GiniG2 = 0.5\n",
    "\n",
    "Gini Index overall = 0.5 + 0.5 = 1\n",
    "\n",
    "This will extend even if we have a multiclass situation and more groups\n",
    "\n",
    "If we have 2 groups, and 4 classes each,\n",
    "a perfectly imperfect split will have a probability of 0.25 in each group\n",
    "\n",
    "Therefore , GiniG1 =  (1 - $(4*{0.25}^{4})$  = 0.984375\n",
    "Similarly , GiniG2 = 0.984375\n",
    "Total Gini = 1.96875\n",
    "\n",
    "## So what does this mean when used in training ?\n",
    "\n",
    "We define Gini Gain, which is Gini Index before splitting - Gini Index after splitting\n",
    "\n",
    "A feature which results in a split with the least gini index  (or equivalently the highest gini gain ) is used as the root node \n",
    "\n",
    "\n",
    "## Ok. Features are selected based on highest gini gain, but how are thresholds selected for continuous features ? Are all possible thresholds tried out ?\n",
    "\n",
    "TBA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to Gini gain\n",
    "\n",
    "The formula for entropy using the same notation  above is the usual shannon entropy formulation\n",
    "\n",
    "Entropy = -$\\sum_{i}{p_{i}log_{2}p_{i}}$\n",
    "\n",
    "where the sum again is over all classes in the group\n",
    "\n",
    "In a perfect split, Entropy = 0, as p_i will be 1 or 0\n",
    "For a non perfect split, entropy will be a positive number\n",
    "\n",
    "Similar to Gini gain, in training, we compute entropy across all groups before and after splitting to and subtract to get information gain\n",
    "\n",
    "We choose the feature to take a split, based on which feature maximizes entropy gain at a given step\n",
    "\n",
    "## Nice properties of information gain\n",
    "\n",
    "1) Information gain is always non-negative (which means if a decision tree uses entropy, it is guaranteed to not become worse at every step, atleast for training data)\n",
    "\n",
    "How ?\n",
    "\n",
    "Assume split is done using feature/input variable Xi\n",
    "Information gain = entropy(before split) - entropy(after split)\n",
    "      = entropy($p_{Y}(D)$) - $\\sum_{j}(fraction of points in group j after splitting by Xi)*entropy(group j)$\n",
    "      \n",
    " entropy($p_{Y}(D)$) simply means that entropy is a function of distribution of label Y in the training data D\n",
    "      \n",
    "Information gain = entropy($p_{Y}(D)$) - $\\sum_{j}(fraction of points in group j)*entropy(p_{Y}(subset of D belongs to group j))$\n",
    "\n",
    "which can be equivalently written in terms of conditional entropy as \n",
    "\n",
    "Information gain = entropy($p_{Y}(D)$) - $entropy(p_{Y}(D) | p_{Xi}(D))$ [see here][5]\n",
    "\n",
    "Term 2 is entropy after split - which is a function of probability distribution of Y over D, given D being split according to split variable Xi which can take m possible values, giving rise to m possible groups after splitting (these m groups are represented by index j in the equation above)\n",
    "\n",
    "Entropy before split is entropy given distribution of Y over training data D,\n",
    "entropy after split is entropy given distribution of Y over training data D conditional on doing a split based on feature $X_{j}$\n",
    "\n",
    "the second term is a relative entropy term\n",
    "\n",
    "\n",
    "Writing this as\n",
    "information gain = Entropy(Q) - Entropy(Q|P)\n",
    "where Q is distribution before split, and Q|P is distribution after split on feature Xj\n",
    "\n",
    "Entropy(Q|P) = $\\sum_{j}q_{j}log_{2}(p_{j}|q_{j})$\n",
    "\n",
    "\n",
    "Expanding this further, assume P and Q are two features with distributions with n and m elements respectively, and we split jointly on both P and Q\n",
    "This creates a table T, where the count in cell i,j (Tij) represents the data which survives split Pi and split Qj. Let pij = Tij/|D| \n",
    "\n",
    "Entropy(Q|P) is the entropy of a record surviving split Q conditional on it already having survived split P\n",
    "\n",
    "Therefore, Entropy(Q|P)\n",
    "\n",
    "\n",
    "\n",
    "How is conditional entropy Entropy(Q|P) defined ?\n",
    "Given a split P first, and then a split q on top of P\n",
    "\n",
    "\n",
    "\n",
    "= -$\\sum_{j}q_{j}log_{2}q_{j}$  +  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ok. Features are selected based on highest entropy gain, but how are thresholds selected for continuous features ? Are all possible thresholds tried out ?\n",
    "\n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini gain vs Entropy gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some decision tree variants such as CART (classification and regression trees) use gini gain . Other variants are ID3 and C4.5 use entropy gain\n",
    "\n",
    "1) Pros of gini gain - Entropy gain needs a log computation, which is more expensive computationally than gini gain\n",
    "2) Pros of entropy gain - symmetric\n",
    "3) Practically, gini gain favors larger partitions, entropy gain smaller partitions\n",
    "4) Entropy  has theoretically better underpinnings - it is non negative (as is gini index) and is symmetric if you switch the target variable and split variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general properties of impurity functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both entropy and gini index are what we call impurity functions, and we've seen that for both, in base case (complete separation of classes in groups), entropy and gini index attain their lowest value of zero\n",
    "\n",
    "In the worst case, we have a uniform distribution of classes in groups, which gives the maximum value of gini index/entropy\n",
    "\n",
    "This is something we want for any impurity function - a non negative value, which is 0 for a perfect split, and is maximum for the worst case split of a uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic training process involves deciding the following aspects\n",
    "1) Selection of attribute splits (splitting criteria)\n",
    "2) Decision of when to stop splitting (stopping criteria)\n",
    "3) Assignment of label to each terminal mode\n",
    "4) Pruning tree if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create root node R\n",
    "\n",
    "2) If stopping criteria already reached, label root note with most common label yi in data set D, exit\n",
    "\n",
    "3) If not, for each input feature Xi,\n",
    "find tests T, which partition data D in D1,D2..Dk in such a way that information gain/gini gain is maximized\n",
    "\n",
    "4) for each partition of data, repeat 1 to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]: https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/\n",
    "[2]: https://www.analyticssteps.com/blogs/what-gini-index-and-information-gain-decision-trees  \n",
    "[3]: https://sites.math.washington.edu/~morrow/336_15/papers/lev.pdf \n",
    "[4]: https://en.wikipedia.org/wiki/Information_gain_in_decision_trees\n",
    "[5]: https://machinelearningmastery.com/information-gain-and-mutual-information/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) \n",
    "2) https://www.analyticssteps.com/blogs/what-gini-index-and-information-gain-decision-trees\n",
    "3) https://sites.math.washington.edu/~morrow/336_15/papers/lev.pdf\n",
    "4) https://en.wikipedia.org/wiki/Information_gain_in_decision_trees\n",
    "5) https://machinelearningmastery.com/information-gain-and-mutual-information/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
