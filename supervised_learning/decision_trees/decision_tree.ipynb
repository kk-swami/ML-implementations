{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are a class of supervised ML algorithms used for both classification and regression . There are several variants of decision trees, of which CART (classification and regression trees) is the most popular . Other variants are ID3 and C4.5\n",
    "\n",
    "Advantages - 1) Easy to interpret (almost think of it as a bunch of if else)\n",
    "Disadvantages - Prone to overfit, for which Bagging/Boosting methods are a popular solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it work ? Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of it like a tree based algorithm, where at every node, a decision using a set of feature is made. \n",
    "\n",
    "During training, the model using gini gain or entropy gain, which feature to use at every single node are decided\n",
    "\n",
    "CART for example uses Gini gain, where ID3 and C4.5 use entropy gain\n",
    "\n",
    "\n",
    "The goal is to keep each split as \"pure\" as possible to achieve classification. For example, in theory, if there are 4 classes, we would like 4 leaf nodes, each leaf node capturing all the sample points of that class\n",
    "\n",
    "\n",
    "Note that this is a non-parametric model, so there is no gradient descent or any such model to find the values of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait a minute, what are gini gain and entropy gain ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini gain and entropy gain are two alternative cost functions used for decision trees. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini index\n",
    "\n",
    "The formula for Gini index is   1 -  $\\sum_{i}{p_{i}}^{2}$ or equivalently,\n",
    "$\\sum_{i}p_i*({1-p_{i}})$ which are the same\n",
    "\n",
    "where the sum is across estimated probabilities of all classes within each group.\n",
    "Then the gini index for all groups are summed together to get overall gini index for the split\n",
    "\n",
    "One estimate of how effective the split was at a node\n",
    "For example, at a node, if based on a condition learnt at a node, the data is split into two groups\n",
    "\n",
    "Say all points with Feature X1 < 5 goes to one node (group G1), and all with X1 >= 5 goes to another node (group G2). Assume only two classes in GT C1 and C2\n",
    "\n",
    "We get a perfect split if G1 and G2 belong to completely different classes\n",
    "\n",
    "## How do we mathematically formulate this ?\n",
    "\n",
    "If pC1G1 is the estimated fraction of points in group G1 belong to class C1 (just the count of all points in G1 belonging to C1 divided by total no of samples in G1) and pC2G1 is estimated fraction of points in group G1 \n",
    "\n",
    "gini index of Group G1  GiniG1= 1 - $({pC1G1}^{2} + {pC2G1}^{2})$\n",
    "\n",
    "Similarly , gini index of Group G2 GiniG2 = 1 - $({pC1G2}^{2} + {pC2G2}^{2})$\n",
    "\n",
    "\n",
    "Overall Gini index = GiniG1 + GiniG2 \n",
    "\n",
    "## So what happens if split is completely perfect or completely imperfect ?\n",
    "\n",
    "For a completely perfect split, pC1G1 = 1, pC2G2=1, pC1G2 = 0, pC2G1 = 0\n",
    "\n",
    "Therefore, GiniG1 = 1 - (1 + 0) = 0\n",
    "GiniG2 = 0\n",
    "\n",
    "Theferefore, Gini Index for a perfect split is 0\n",
    "\n",
    "\n",
    "For a completely imperfect split, all 4 probabilities will be 0.5\n",
    "\n",
    "GiniG1 = (1 - $({0.5}^{2} + {0.5}^{2})$ = 1 - 0.5 = 0.5\n",
    "Similarly, GiniG2 = 0.5\n",
    "\n",
    "Gini Index overall = 0.5 + 0.5 = 1\n",
    "\n",
    "This will extend even if we have a multiclass situation and more groups\n",
    "\n",
    "If we have 2 groups, and 4 classes each,\n",
    "a perfectly imperfect split will have a probability of 0.25 in each group\n",
    "\n",
    "Therefore , GiniG1 =  (1 - $(4*{0.25}^{4})$  = 0.984375\n",
    "Similarly , GiniG2 = 0.984375\n",
    "Total Gini = 1.96875\n",
    "\n",
    "## So what does this mean when used in training ?\n",
    "\n",
    "We define Gini Gain, which is Gini Index before splitting - Gini Index after splitting\n",
    "\n",
    "A feature which results in a split with the least gini index  (or equivalently the highest gini gain ) is used as the root node \n",
    "\n",
    "\n",
    "## Ok. Features are selected based on highest gini gain, but how are thresholds selected for continuous features ? Are all possible thresholds tried out ?\n",
    "\n",
    "Yes kind of - look at implementation below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to Gini gain\n",
    "\n",
    "The formula for entropy using the same notation  above is the usual shannon entropy formulation\n",
    "\n",
    "Entropy = -$\\sum_{i}{p_{i}log_{2}p_{i}}$\n",
    "\n",
    "where the sum again is over all classes in the group\n",
    "\n",
    "In a perfect split, Entropy = 0, as p_i will be 1 or 0\n",
    "For a non perfect split, entropy will be a positive number\n",
    "\n",
    "Similar to Gini gain, in training, we compute entropy across all groups before and after splitting to and subtract to get information gain\n",
    "\n",
    "We choose the feature to take a split, based on which feature maximizes entropy gain at a given step\n",
    "\n",
    "## Nice properties of information gain\n",
    "\n",
    "1) Information gain is always non-negative (which means if a decision tree uses entropy, it is guaranteed to not become worse at every step, atleast for training data)\n",
    "\n",
    "How ?\n",
    "\n",
    "Assume split is done using feature/input variable Xi\n",
    "Information gain = entropy(before split) - entropy(after split)\n",
    "      = entropy($p_{Y}(D)$) - $\\sum_{j}(fraction of points in group j after splitting by Xi)*entropy(group j)$\n",
    "      \n",
    " entropy($p_{Y}(D)$) simply means that entropy is a function of distribution of label Y in the training data D\n",
    "      \n",
    "Information gain = entropy($p_{Y}(D)$) - $\\sum_{j}(fraction of points in group j)*entropy(p_{Y}(subset of D belongs to group j))$\n",
    "\n",
    "which can be equivalently written in terms of conditional entropy as \n",
    "\n",
    "Information gain = entropy($p_{Y}(D)$) - $entropy(p_{Y}(D) | p_{Xi}(D))$ [see here][5]\n",
    "\n",
    "Term 2 is entropy after split - which is a function of probability distribution of Y over D, given D being split according to split variable Xi which can take m possible values, giving rise to m possible groups after splitting (these m groups are represented by index j in the equation above)\n",
    "\n",
    "Entropy before split is entropy given distribution of Y over training data D,\n",
    "entropy after split is entropy given distribution of Y over training data D conditional on doing a split based on feature $X_{j}$\n",
    "\n",
    "the second term is a relative entropy term\n",
    "\n",
    "\n",
    "Writing this as\n",
    "information gain = Entropy(Q) - Entropy(Q|P)\n",
    "where Q is distribution before split, and Q|P is distribution after split on feature Xj\n",
    "\n",
    "Entropy(Q|P) = $\\sum_{j}q_{j}log_{2}(p_{j}|q_{j})$\n",
    "\n",
    "\n",
    "Expanding this further, assume P and Q are two features with distributions with n and m elements respectively, and we split jointly on both P and Q\n",
    "This creates a table T, where the count in cell i,j (Tij) represents the data which survives split Pi and split Qj. Let pij = Tij/|D| \n",
    "\n",
    "Entropy(Q|P) is the entropy of a record surviving split Q conditional on it already having survived split P\n",
    "\n",
    "Therefore, Entropy(Q|P)\n",
    "\n",
    "\n",
    "\n",
    "How is conditional entropy Entropy(Q|P) defined ?\n",
    "Given a split P first, and then a split q on top of P\n",
    "\n",
    "\n",
    "\n",
    "= -$\\sum_{j}q_{j}log_{2}q_{j}$  +  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ok. Features are selected based on highest entropy gain, but how are thresholds selected for continuous features ? Are all possible thresholds tried out ?\n",
    "\n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini gain vs Entropy gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some decision tree variants such as CART (classification and regression trees) use gini gain . Other variants are ID3 and C4.5 use entropy gain\n",
    "\n",
    "1) Pros of gini gain - Entropy gain needs a log computation, which is more expensive computationally than gini gain\n",
    "2) Pros of entropy gain - symmetric\n",
    "3) Practically, gini gain favors larger partitions, entropy gain smaller partitions\n",
    "4) Entropy  has theoretically better underpinnings - it is non negative (as is gini index) and is symmetric if you switch the target variable and split variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general properties of impurity functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both entropy and gini index are what we call impurity functions, and we've seen that for both, in base case (complete separation of classes in groups), entropy and gini index attain their lowest value of zero\n",
    "\n",
    "In the worst case, we have a uniform distribution of classes in groups, which gives the maximum value of gini index/entropy\n",
    "\n",
    "This is something we want for any impurity function - a non negative value, which is 0 for a perfect split, and is maximum for the worst case split of a uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic training process involves deciding the following aspects\n",
    "1) Selection of attribute splits (splitting criteria)\n",
    "2) Decision of when to stop splitting (stopping criteria)\n",
    "3) Assignment of label to each terminal mode\n",
    "4) Pruning tree if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Create root node R\n",
    "\n",
    "2) If stopping criteria already reached, label root note with most common label yi in data set D, exit\n",
    "\n",
    "3) If not, for each input feature Xi,\n",
    "find tests T, which partition data D in D1,D2..Dk in such a way that information gain/gini gain is maximized\n",
    "\n",
    "4) for each partition of data, repeat 1 to 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>174</td>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>189</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>185</td>\n",
       "      <td>110</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>195</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>149</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Height  Weight  Index\n",
       "0    Male     174      96      4\n",
       "1    Male     189      87      2\n",
       "2  Female     185     110      4\n",
       "3  Female     195     104      3\n",
       "4    Male     149      61      3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"500_Person_Gender_Height_Weight_Index.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict whether a person is obese or not.\n",
    "We are saying a person with weight index 3 or 4 is obese. That is our label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['obese'] = (data.Index >= 4).astype('int')\n",
    "data.drop('Index', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we define a particular rule in the tree, as patient with weight >= 100 kg are obese.\n",
    "Then , for each of the two subtrees thus obtained, if any one of the subtrees had all patients as obese or all patients as not obese, the impurity function is 0 (either gini or entropy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Misclassified when cutting at 100kg: 18 \n",
      " Misclassified when cutting at 80kg: 63\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "  \" Misclassified when cutting at 100kg:\",\n",
    "  data.loc[(data['Weight']>=100) & (data['obese']==0),:].shape[0], \"\\n\",\n",
    "  \"Misclassified when cutting at 80kg:\",\n",
    "  data.loc[(data['Weight']>=80) & (data['obese']==0),:].shape[0]\n",
    ")\n",
    "\n",
    "## This means both the splitting at 100 and splitting at 80 results in impure partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate impurity using gini index and entropy which are two alternative methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gini_impurity(y):\n",
    "  '''\n",
    "  Given a Pandas Series, it calculates the Gini Impurity. \n",
    "  y: variable with which calculate Gini Impurity.\n",
    "  '''\n",
    "  if isinstance(y, pd.Series):\n",
    "    p = y.value_counts()/y.shape[0]\n",
    "    gini = 1-np.sum(p**2)\n",
    "    return(gini)\n",
    "\n",
    "  else:\n",
    "    raise('Object must be a Pandas Series.')\n",
    "\n",
    "gini_impurity(data.Gender) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997114388674198"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def entropy(y):\n",
    "  '''\n",
    "  Given a Pandas Series, it calculates the entropy. \n",
    "  y: variable with which calculate entropy.\n",
    "  '''\n",
    "  if isinstance(y, pd.Series):\n",
    "    a = y.value_counts()/y.shape[0]\n",
    "    entropy = np.sum(-a*np.log2(a+1e-9)) ## the 10-9 is to avoid situations where p is 0\n",
    "    return(entropy)\n",
    "\n",
    "  else:\n",
    "    raise('Object must be a Pandas Series.')\n",
    "\n",
    "entropy(data.Gender)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define information gain for any split as entropy before splitting - entropy after splitting.\n",
    "\n",
    "(We can also use gini index before splitting - gini afterwards, but here we are using entropy)\n",
    "\n",
    "For regression, we instead use variance before splitting - variance after splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(y):\n",
    "  '''\n",
    "  Function to help calculate the variance avoiding nan.\n",
    "  y: variable to calculate variance to. It should be a Pandas Series.\n",
    "  '''\n",
    "  if(len(y) == 1):\n",
    "    return 0\n",
    "  else:\n",
    "    return y.var()\n",
    "\n",
    "def information_gain(y, mask, func=entropy):\n",
    "  '''\n",
    "  It returns the Information Gain of a variable given a loss function.\n",
    "  y: target variable.\n",
    "  mask: split choice.\n",
    "  func: function to be used to calculate Information Gain in case os classification.\n",
    "  '''\n",
    "  \n",
    "  a = sum(mask)\n",
    "  b = mask.shape[0] - a\n",
    "  \n",
    "  if(a == 0 or b ==0): \n",
    "    ig = 0\n",
    "  \n",
    "  else:\n",
    "    if y.dtypes != 'O':\n",
    "      ig = variance(y) - (a/(a+b)* variance(y[mask])) - (b/(a+b)*variance(y[-mask]))\n",
    "    else:\n",
    "      ig = func(y)-a/(a+b)*func(y[mask])-b/(a+b)*func(y[-mask])\n",
    "  \n",
    "  return ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0002808244603327431"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain(data['obese'], data['Gender']=='Male')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it negative ? Information gain by definition cannot be negative (see above). Could be rounding errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. so how do we now choose the split which maximizes information gain ?\n",
    "\n",
    "If feature is numeric, we try thresholds for all possible values of the feature,\n",
    "for each threshold, we apply a condition where if f<=t, we call it 0, else 1, we compute \n",
    "for each of these conditions information gain, and see what threshold maximizes it\n",
    "\n",
    "For categorical values, we have to compute information gain for all possible values \n",
    "of that category \n",
    "If the number of possible categories is too much,  this leads to a combinatorial explosion, so we typically restrict to <= 20 categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def categorical_options(a):\n",
    "  '''\n",
    "  Creates all possible combinations from a Pandas Series.\n",
    "  a: Pandas Series from where to get all possible combinations. \n",
    "  '''\n",
    "  a = a.unique()\n",
    "\n",
    "  opciones = []\n",
    "  for L in range(0, len(a)+1):\n",
    "      for subset in itertools.combinations(a, L):\n",
    "          subset = list(subset)\n",
    "          opciones.append(subset)\n",
    "\n",
    "  return opciones[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame({'value' : ['a']*5 + ['b']*6 + ['c']*7 + ['d']*8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a'],\n",
       " ['b'],\n",
       " ['c'],\n",
       " ['d'],\n",
       " ['a', 'b'],\n",
       " ['a', 'c'],\n",
       " ['a', 'd'],\n",
       " ['b', 'c'],\n",
       " ['b', 'd'],\n",
       " ['c', 'd'],\n",
       " ['a', 'b', 'c'],\n",
       " ['a', 'b', 'd'],\n",
       " ['a', 'c', 'd'],\n",
       " ['b', 'c', 'd']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_options(a.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best split for Weight is when the variable is less than  103 \n",
      "Information Gain for that split is: 0.10625190497954848\n"
     ]
    }
   ],
   "source": [
    "def max_information_gain_split(x, y, func=entropy):\n",
    "  '''\n",
    "  Given a predictor & target variable, returns the best split, the error and the type of variable based on a selected cost function.\n",
    "  x: predictor variable as Pandas Series.\n",
    "  y: target variable as Pandas Series.\n",
    "  func: function to be used to calculate the best split.\n",
    "  '''\n",
    "\n",
    "  split_value = []\n",
    "  ig = [] \n",
    "\n",
    "  numeric_variable = True if x.dtypes != 'O' else False\n",
    "\n",
    "  # Create options according to variable type\n",
    "  if numeric_variable:\n",
    "    options = x.sort_values().unique()[1:]\n",
    "  else: \n",
    "    options = categorical_options(x)\n",
    "\n",
    "  # Calculate ig for all values\n",
    "  for val in options:\n",
    "    mask =   x < val if numeric_variable else x.isin(val)\n",
    "    val_ig = information_gain(y, mask, func)\n",
    "    # Append results\n",
    "    ig.append(val_ig)\n",
    "    split_value.append(val)\n",
    "\n",
    "  # Check if there are more than 1 results if not, return False\n",
    "  if len(ig) == 0:\n",
    "    return(None,None,None, False)\n",
    "\n",
    "  else:\n",
    "  # Get results with highest IG\n",
    "    best_ig = max(ig)\n",
    "    best_ig_index = ig.index(best_ig)\n",
    "    best_split = split_value[best_ig_index]\n",
    "    return(best_ig,best_split,numeric_variable, True)\n",
    "\n",
    "\n",
    "weight_ig, weight_slpit, _, _ = max_information_gain_split(data['Weight'], data['obese'],)  \n",
    "\n",
    "\n",
    "print(\n",
    "  \"The best split for Weight is when the variable is less than \",\n",
    "  weight_slpit,\"\\nInformation Gain for that split is:\", weight_ig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for one feature, we repeat for each feature to see the feature contributing to best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000281</td>\n",
       "      <td>0.019684</td>\n",
       "      <td>0.106252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Male]</td>\n",
       "      <td>174</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gender    Height    Weight\n",
       "0 -0.000281  0.019684  0.106252\n",
       "1    [Male]       174       103\n",
       "2     False      True      True\n",
       "3      True      True      True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop('obese', axis= 1).apply(max_information_gain_split, y = data['obese'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable with the highest Information Gain is Weight. Therefore, it will be the variable that we use first to do the split. In addition, we also have the value on which the split must be performed: 103.\n",
    "\n",
    "With this, we already have the first split, which would generate two dataframes. If we apply this recursively, we will end up creating the entire decision tree (coded in Python from scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete implementation - Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth: maximum depth of the tree. If we set it to None, the tree will grow until all the leaves are pure or the hyperparameter min_samples_split has been reached.\n",
    "min_samples_split: indicates the minimum number of observations a sheet must have to continue creating new nodes.\n",
    "min_information_gain: the minimum amount the Information Gain must increase for the tree to continue growing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(y, data):\n",
    "  '''\n",
    "  Given a data, select the best split and return the variable, the value, the variable type and the information gain.\n",
    "  y: name of the target variable\n",
    "  data: dataframe where to find the best split.\n",
    "  '''\n",
    "  masks = data.drop(y, axis= 1).apply(max_information_gain_split, y = data[y])\n",
    "  if sum(masks.loc[3,:]) == 0:\n",
    "    return(None, None, None, None)\n",
    "\n",
    "  else:\n",
    "    # Get only masks that can be splitted\n",
    "    masks = masks.loc[:,masks.loc[3,:]]\n",
    "\n",
    "    # Get the results for split with highest IG\n",
    "    split_variable = max(masks)\n",
    "    #split_valid = masks[split_variable][]\n",
    "    split_value = masks[split_variable][1] \n",
    "    split_ig = masks[split_variable][0]\n",
    "    split_numeric = masks[split_variable][2]\n",
    "\n",
    "    return(split_variable, split_value, split_ig, split_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_split(variable, value, data, is_numeric):\n",
    "  '''\n",
    "  Given a data and a split conditions, do the split.\n",
    "  variable: variable with which make the split.\n",
    "  value: value of the variable to make the split.\n",
    "  data: data to be splitted.\n",
    "  is_numeric: boolean considering if the variable to be splitted is numeric or not.\n",
    "  '''\n",
    "  if is_numeric:\n",
    "    data_1 = data[data[variable] < value]\n",
    "    data_2 = data[(data[variable] < value) == False]\n",
    "\n",
    "  else:\n",
    "    data_1 = data[data[variable].isin(value)]\n",
    "    data_2 = data[(data[variable].isin(value)) == False]\n",
    "\n",
    "  return(data_1,data_2)\n",
    "\n",
    "def make_prediction(data, target_factor):\n",
    "  '''\n",
    "  Given the target variable, make a prediction.\n",
    "  data: pandas series for target variable\n",
    "  target_factor: boolean considering if the variable is a factor or not\n",
    "  '''\n",
    "\n",
    "  # Make predictions\n",
    "  if target_factor:\n",
    "    pred = data.value_counts().idxmax()\n",
    "  else:\n",
    "    pred = data.mean()\n",
    "\n",
    "  return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Weight <=  103': [{'Weight <=  74': [0,\n",
       "    {'Weight <=  84': [{'Weight <=  75': [1, 0]},\n",
       "      {'Weight <=  98': [1, 0]}]}]},\n",
       "  1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_tree(data,y, target_factor, max_depth = None,min_samples_split = None, min_information_gain = 1e-20, counter=0, max_categories = 20):\n",
    "  '''\n",
    "  Trains a Decission Tree\n",
    "  data: Data to be used to train the Decission Tree\n",
    "  y: target variable column name\n",
    "  target_factor: boolean to consider if target variable is factor or numeric.\n",
    "  max_depth: maximum depth to stop splitting.\n",
    "  min_samples_split: minimum number of observations to make a split.\n",
    "  min_information_gain: minimum ig gain to consider a split to be valid.\n",
    "  max_categories: maximum number of different values accepted for categorical values. High number of values will slow down learning process. R\n",
    "  '''\n",
    "\n",
    "  # Check that max_categories is fulfilled\n",
    "  if counter==0:\n",
    "    types = data.dtypes\n",
    "    check_columns = types[types == \"object\"].index\n",
    "    for column in check_columns:\n",
    "      var_length = len(data[column].value_counts()) \n",
    "      if var_length > max_categories:\n",
    "        raise ValueError('The variable ' + column + ' has '+ str(var_length) + ' unique values, which is more than the accepted ones: ' +  str(max_categories))\n",
    "\n",
    "  # Check for depth conditions\n",
    "  if max_depth == None:\n",
    "    depth_cond = True\n",
    "\n",
    "  else:\n",
    "    if counter < max_depth:\n",
    "      depth_cond = True\n",
    "\n",
    "    else:\n",
    "      depth_cond = False\n",
    "\n",
    "  # Check for sample conditions\n",
    "  if min_samples_split == None:\n",
    "    sample_cond = True\n",
    "\n",
    "  else:\n",
    "    if data.shape[0] > min_samples_split:\n",
    "      sample_cond = True\n",
    "\n",
    "    else:\n",
    "      sample_cond = False\n",
    "\n",
    "  # Check for ig condition\n",
    "  if depth_cond & sample_cond:\n",
    "\n",
    "    var,val,ig,var_type = get_best_split(y, data)\n",
    "\n",
    "    # If ig condition is fulfilled, make split \n",
    "    if ig is not None and ig >= min_information_gain:\n",
    "\n",
    "      counter += 1\n",
    "\n",
    "      left,right = make_split(var, val, data,var_type)\n",
    "\n",
    "      # Instantiate sub-tree\n",
    "      split_type = \"<=\" if var_type else \"in\"\n",
    "      question =   \"{} {}  {}\".format(var,split_type,val)\n",
    "      # question = \"\\n\" + counter*\" \" + \"|->\" + var + \" \" + split_type + \" \" + str(val) \n",
    "      subtree = {question: []}\n",
    "\n",
    "\n",
    "      # Find answers (recursion)\n",
    "      yes_answer = train_tree(left,y, target_factor, max_depth,min_samples_split,min_information_gain, counter)\n",
    "\n",
    "      no_answer = train_tree(right,y, target_factor, max_depth,min_samples_split,min_information_gain, counter)\n",
    "\n",
    "      if yes_answer == no_answer:\n",
    "        subtree = yes_answer\n",
    "\n",
    "      else:\n",
    "        subtree[question].append(yes_answer)\n",
    "        subtree[question].append(no_answer)\n",
    "\n",
    "    # If it doesn't match IG condition, make prediction\n",
    "    else:\n",
    "      pred = make_prediction(data[y],target_factor)\n",
    "      return pred\n",
    "\n",
    "   # Drop dataset if doesn't match depth or sample conditions\n",
    "  else:\n",
    "    pred = make_prediction(data[y],target_factor)\n",
    "    return pred\n",
    "\n",
    "  return subtree\n",
    "\n",
    "\n",
    "max_depth = 5\n",
    "min_samples_split = 20\n",
    "min_information_gain  = 1e-5\n",
    "\n",
    "\n",
    "decisiones = train_tree(data,'obese',True, max_depth,min_samples_split,min_information_gain)\n",
    "\n",
    "\n",
    "decisiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using our decision tree in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificar_datos(observacion, arbol):\n",
    "  question = list(arbol.keys())[0] \n",
    "\n",
    "  if question.split()[1] == '<=':\n",
    "\n",
    "    if observacion[question.split()[0]] <= float(question.split()[2]):\n",
    "      answer = arbol[question][0]\n",
    "    else:\n",
    "      answer = arbol[question][1]\n",
    "\n",
    "  else:\n",
    "\n",
    "    if observacion[question.split()[0]] in (question.split()[2]):\n",
    "      answer = arbol[question][0]\n",
    "    else:\n",
    "      answer = arbol[question][1]\n",
    "\n",
    "  # If the answer is not a dictionary\n",
    "  if not isinstance(answer, dict):\n",
    "    return answer\n",
    "  else:\n",
    "    residual_tree = answer\n",
    "    return clasificar_datos(observacion, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ensure that both min_samples_split and max_depth are fulfilled.\n",
    "If they are fulfilled, we get the best split and obtain the Information Gain. If any of the conditions are not fulfilled, we make the prediction.\n",
    "We check that the Information Gain Comprobamos passes the minimum amount set by min_information_gain.\n",
    "If the condition above is fulfilled, we make the split and save the decision. If it is not fulfilled, then we make the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]: https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/\n",
    "[2]: https://www.analyticssteps.com/blogs/what-gini-index-and-information-gain-decision-trees  \n",
    "[3]: https://sites.math.washington.edu/~morrow/336_15/papers/lev.pdf \n",
    "[4]: https://en.wikipedia.org/wiki/Information_gain_in_decision_trees\n",
    "[5]: https://machinelearningmastery.com/information-gain-and-mutual-information/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) \n",
    "2) https://www.analyticssteps.com/blogs/what-gini-index-and-information-gain-decision-trees\n",
    "3) https://sites.math.washington.edu/~morrow/336_15/papers/lev.pdf\n",
    "4) https://en.wikipedia.org/wiki/Information_gain_in_decision_trees\n",
    "5) https://machinelearningmastery.com/information-gain-and-mutual-information/\n",
    "6) https://anderfernandez.com/en/blog/code-decision-tree-python-from-scratch/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
