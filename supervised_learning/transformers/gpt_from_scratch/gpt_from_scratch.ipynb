{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f2fa50-0ded-45ba-a98c-3aec9eccfa30",
   "metadata": {},
   "source": [
    "# GPT from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27884165-a28a-4e41-ab7c-a0290ca9058f",
   "metadata": {},
   "source": [
    "From Karpathy https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "The aim is to build a character level nano-GPT model (not word level) since its simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e393ed-8ee5-4337-91bc-092ea60157ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "## Download the tiny shakespeare dataset from this URL\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fc10fb-c23c-41d0-af38-ed3a7dce7775",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../../../data/tinyshakespeare/input.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c08618-15e9-4d19-afd7-56cf1d6c50a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3a5d69f-a6ac-484c-bf1c-48381a51e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_path, 'r', encoding='utf-8') as fp:\n",
    "    text = fp.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e2ae2e0-64a7-4645-b6f2-569b6d9cc507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "print(f'The text has {len(text)} characters')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19ec3154-cfdc-49ad-bb83-78d9a91acfad",
   "metadata": {},
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ce196e-4c38-4534-b937-cf291882e6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "## Let's get the unique characters and vocab. Treats upper case and lower case differently.\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18fa5399-f61b-4dba-80c8-165bc0e29018",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization : Since its a character level language model, tokenization is at character level\n",
    "## Convert vocab to set of indices\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } ## st to index\n",
    "itos = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eee27b94-c6fe-44b1-b0ef-10c124857680",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define encode (takes a string , gives a list of integers) and decode (takes list of integers, gives a string) lambda functions\n",
    "encode = lambda s : [stoi[x] for x in s]\n",
    "decode = lambda l : \"\".join([itos[x] for x in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d13e44ea-ad5a-4009-a88c-ba9a2c39e26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hi there\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9312ba7-cf96-40a6-9303-b6f35bb55aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there\n"
     ]
    }
   ],
   "source": [
    "print(decode([46, 47, 1, 58, 46, 43, 56, 43]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60bb7de-0b50-47b2-87fe-7557f7b56cf5",
   "metadata": {},
   "source": [
    "## Encode the whole corpus, and convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8885d10d-57d7-4a6b-854e-cb5b4a61f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long) ## torch.long is 64 bit integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64f6bc55-93eb-4437-b4ba-a1b4f6cc5eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "941b7aba-44cb-43ab-9469-53f45f329af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb78ef-a05d-438f-90fa-39a2b61ba0bc",
   "metadata": {},
   "source": [
    "## Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8bc58910-cf7d-47a3-8ac4-b75baa538013",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) ## First 90% train, 10% validation. Since its a language model we have to keep continuous chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b7ea95c-0d24-496f-979b-54fb73fd306a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003854"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc8be4c2-1108-4b25-a8ee-0b11ac57166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[0:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45170e7a-1135-4c50-a55a-b339883cd568",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8 ## length of each chunk fed to the NN, not too large, is it ? (context window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a02aa58-4bb7-4fb1-8159-93845a63b51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), target is 47\n",
      "When input is tensor([18, 47]), target is 56\n",
      "When input is tensor([18, 47, 56]), target is 57\n",
      "When input is tensor([18, 47, 56, 57]), target is 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), target is 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), target is 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), target is 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] ## Initial input\n",
    "y = train_data[1:block_size + 1] ## target (offset by 1 from input)\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] ## Why t+1 and not t ? If you work out the details it will come out\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context}, target is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd770230-9916-433a-bce5-3be76b98b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So we get 8 training examples from the first block of 9 characters  - 8 inputs + 1 target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdb78a-daa6-4c9f-a3dd-ba2ff5c260a6",
   "metadata": {},
   "source": [
    "## Now we want to do batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7514e0d9-2418-460f-877d-109d4b4b555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4f76d71-eed9-4210-8efd-8b311d865169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data =  train_data if split=='train' else val_data\n",
    "    ix = torch.randint(0,len(data)-block_size, (batch_size,)) ## picks batch_size no random integers between low and high\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix], dim=0) ## row concatenation\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix], dim=0) ## row concatenation\n",
    "    \n",
    "    return x,y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22ef112e-0d4d-4eef-b916-069bee515649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[52, 57,  8,  0, 21,  5, 50, 50],\n",
      "        [47, 58, 46,  1, 63, 53, 59,  6],\n",
      "        [ 1, 53, 40, 43, 63, 12,  0, 26],\n",
      "        [50, 42, 57, 58,  6,  0, 32, 46]])\n",
      "targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[57,  8,  0, 21,  5, 50, 50,  1],\n",
      "        [58, 46,  1, 63, 53, 59,  6,  1],\n",
      "        [53, 40, 43, 63, 12,  0, 26, 39],\n",
      "        [42, 57, 58,  6,  0, 32, 46, 43]])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(\"inputs: \")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets: \")\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1dcc89-775b-4e99-833e-1ffad0524a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This 4*8 array contains 32 examples which are independent for transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4a34004-9317-4e11-9aef-7872bb647c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([52]), target is 57\n",
      "When input is tensor([52, 57]), target is 8\n",
      "When input is tensor([52, 57,  8]), target is 0\n",
      "When input is tensor([52, 57,  8,  0]), target is 21\n",
      "When input is tensor([52, 57,  8,  0, 21]), target is 5\n",
      "When input is tensor([52, 57,  8,  0, 21,  5]), target is 50\n",
      "When input is tensor([52, 57,  8,  0, 21,  5, 50]), target is 50\n",
      "When input is tensor([52, 57,  8,  0, 21,  5, 50, 50]), target is 1\n",
      "When input is tensor([47]), target is 58\n",
      "When input is tensor([47, 58]), target is 46\n",
      "When input is tensor([47, 58, 46]), target is 1\n",
      "When input is tensor([47, 58, 46,  1]), target is 63\n",
      "When input is tensor([47, 58, 46,  1, 63]), target is 53\n",
      "When input is tensor([47, 58, 46,  1, 63, 53]), target is 59\n",
      "When input is tensor([47, 58, 46,  1, 63, 53, 59]), target is 6\n",
      "When input is tensor([47, 58, 46,  1, 63, 53, 59,  6]), target is 1\n",
      "When input is tensor([1]), target is 53\n",
      "When input is tensor([ 1, 53]), target is 40\n",
      "When input is tensor([ 1, 53, 40]), target is 43\n",
      "When input is tensor([ 1, 53, 40, 43]), target is 63\n",
      "When input is tensor([ 1, 53, 40, 43, 63]), target is 12\n",
      "When input is tensor([ 1, 53, 40, 43, 63, 12]), target is 0\n",
      "When input is tensor([ 1, 53, 40, 43, 63, 12,  0]), target is 26\n",
      "When input is tensor([ 1, 53, 40, 43, 63, 12,  0, 26]), target is 39\n",
      "When input is tensor([50]), target is 42\n",
      "When input is tensor([50, 42]), target is 57\n",
      "When input is tensor([50, 42, 57]), target is 58\n",
      "When input is tensor([50, 42, 57, 58]), target is 6\n",
      "When input is tensor([50, 42, 57, 58,  6]), target is 0\n",
      "When input is tensor([50, 42, 57, 58,  6,  0]), target is 32\n",
      "When input is tensor([50, 42, 57, 58,  6,  0, 32]), target is 46\n",
      "When input is tensor([50, 42, 57, 58,  6,  0, 32, 46]), target is 43\n"
     ]
    }
   ],
   "source": [
    "for batch in range(batch_size):\n",
    "    for time in range(block_size):\n",
    "        context = xb[batch,:time+1]\n",
    "        target = yb[batch,time]\n",
    "        print(f\"When input is {context}, target is {target}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c69c8-fbf3-4143-8609-6f2bb4b014df",
   "metadata": {},
   "source": [
    "## Bigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a1c8f4a-c775-453b-9848-45bcb9515ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "14921dde-2cc9-477a-bdf3-43e8b63c677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off logits of next token using a lookup\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) ## interestingly, the embedding dimension is set to the vocab size itself ?\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        ## idx and targets are both B, T tensor of integers (batch size, window size)\n",
    "        logits = self.token_embedding_table(idx) ## dimension B,T,C -> C is no of channels, which is vocab size here\n",
    "        ## Logits are just the predictions ! We want to compute cross entropy loss between predictions and targets using F.cross_entropy\n",
    "        ## But hold on : F.cross_entropy expects in form B*T, C\n",
    "        ## Therefore reshape !!\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "        \n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    def generate(self, idx, max_new_tokens ):\n",
    "        # idx is B,T array\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on last time step\n",
    "            logits = logits[:,-1,:] ## Becomes B,C\n",
    "            \n",
    "            ## softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1) ## Dimension applied across C,B*T*C ->  B*C dimension\n",
    "            \n",
    "            ## sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) ## B*1 dimension\n",
    "            \n",
    "            ## append sampled index to running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1 ) # B*T+1\n",
    "        return idx\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "743f5aed-129f-4421-a334-cc5c8f7e28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BigramLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b6058a1-7634-4367-9aab-36e73ca2a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, loss = m(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d945deed-a2d6-4e03-a4d6-92d3ab26f967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8720, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "838d21dc-b878-491f-87e4-9ffa8dfe3a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 65])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c0f7eff-e391-415b-aa88-17fb2f22b606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "207273c6-1143-44eb-b320-75f4be941183",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's now generate using this\n",
    "start_input = torch.zeros([1,1], dtype = torch.long) ## like a batch of 1 sample, of 1 index. Why 0 ? because if you look above, the index 0 represents /n which is a good starting point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ab4aacef-dfad-46a4-9437-5efaec3b141c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n&rK?yNeJDnb\\nQvwVQKR:YQgca.sp,xagkkT,mw:gNoKnRlEza!Quwkgd&RZYHVoXYQ&uLbMJBrlyUdrYXb y -cPe.JfxOVKtdoA'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(list(m.generate(idx = start_input, max_new_tokens= 100 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77fb8d-92d3-4a03-ad19-976857911301",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rubbish as expected because the model is not trained yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f0beb-d029-432d-870c-64ccfe532ec1",
   "metadata": {},
   "source": [
    "## Training the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "714c00ac-3e34-4641-a7c9-03b2099e3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c7d13224-260b-4ba2-bd9a-6df17cbdcd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4409520626068115\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "    \n",
    "    xb, yb = get_batch(\"train\") ## sample a batch of data\n",
    "    \n",
    "    ## evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4328b724-32aa-40f9-bdcc-88bffd65e5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIIZEcon yofy orirl I imondareyo yoovethoves four it burengonond\\nWere ante bencusamy wink houge serst'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## after training, lets just check how well the bigram model does\n",
    "decode(list(m.generate(idx = start_input, max_new_tokens= 100 )[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383fcab-3b6b-4a65-b91b-316f6ab09355",
   "metadata": {},
   "source": [
    "slightly better !! But since it looks only at one token to predict the next one , it can only do so much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7798420-33a3-4aab-9d98-a3fa8259ea1c",
   "metadata": {},
   "source": [
    "## self attention - mathematical trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d85731b4-ec5f-4615-a864-e112506eb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 ## batch size, sequence size, channel/embedding size\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f5921ae0-ba43-4f41-96c3-c1c6ea9188a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040879ed-a8c2-4694-8cab-fd864b3efd6c",
   "metadata": {},
   "source": [
    "We want to couple each of the 8 positions with all other current and past positions using self-attention, not future\n",
    "\n",
    "How do we do this ?\n",
    "\n",
    "Let's take a simple coupling mechanism - every token communicates with all current and past tokens by taking an average of the embeddings of all current and past embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743a6ff-ac30-42c0-801a-220e69baffbd",
   "metadata": {},
   "source": [
    "Implement by 2 for loops - this is not an efficient implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7e8456ec-69b1-43d0-832a-6be6d55435c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] ## of dimension (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0) ## C*\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6ac4e924-57c0-47f4-a563-86e37e1567f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Implement using matrix multiplication - efficient multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41a2e2-a20f-44dd-a76b-4cec4a51bdba",
   "metadata": {},
   "source": [
    "Given a matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0e5ede60-391a-4568-96cf-4afa4321a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "b = torch.randint(0,10, (3,2)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7e7d63d9-e008-45ab-958d-a36dc274de80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 7.],\n",
       "        [6., 4.],\n",
       "        [6., 5.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46fa85-3594-4a79-b4cd-1240f624e8fc",
   "metadata": {},
   "source": [
    "Lets say I want to create a matrix where every row is the average of all rows until that point\n",
    "\n",
    "ie - row 1  is [2,7] as is, row 2 is average of [2,7] and [6,4] and row 3 is average of [2,7], [6,4] and [6,5]\n",
    "\n",
    "So get a matrix with same dimension as b, such that each row is the average of all rows till that point\n",
    "How do you implement this using matrix mulplication ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b067ef4b-6ff0-4187-ade8-0a4bc485bddd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Create a matrix a of dimensiton 3,3 (ie with dimension such that a @ b will have the dimension of b which is what we want )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1a63aac3-7a64-47c7-a4a5-07cd382f2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "63310795-1c06-44ed-ade2-859f921a6b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9a1f0587-f6c9-44a0-bb33-6fa675fa0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a@b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2fa44-d2e5-4fe8-894b-e7fe4f61edec",
   "metadata": {},
   "source": [
    "c has each row as the sum across rows of corresponding column in b , as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2aed2-4dfd-470c-88a5-7fe51253e849",
   "metadata": {},
   "source": [
    "This means each row in b is attending to all other rows in b, not only rows in current and past positions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47d8b1-b0c9-45a1-b51b-35191f00167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Instead of matrix of ones for a, create a lower triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4e134333-88b2-479d-928b-bbff3b7374fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "938df6f1-335d-4ae8-8182-257ad9777bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "71ae5b86-094a-40f6-941d-2ecc38f14b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "86b68b27-c13f-44c1-8644-ca72c685a09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "21b2eb5a-0163-404b-8c8b-49993352cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now you see that every row in c contains sums of all rows in b till that row, so it only attends to current and past positions !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70758b9c-c202-4951-9525-4129bcc1f967",
   "metadata": {},
   "source": [
    "Now we just need to convert the sum to an average\n",
    "to do this, we normalize a, the triangular matrix such that sum of each row is 1 (sum across columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3d56e7a7-0669-448f-81fc-ab979fd7b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a / a.sum(dim=1, keepdim=True)## note that keepdim=True is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1ed7a171-e92a-4396-b1b8-02edb3383c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "58002669-8970-44e2-8511-1486f15d3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a@b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9ef3df46-87bf-4fec-9058-87d8c1a08d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 7.0000],\n",
       "        [4.0000, 5.5000],\n",
       "        [4.6667, 5.3333]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a169d9d7-3f25-4b43-b0c7-536247717185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 7.],\n",
       "        [6., 4.],\n",
       "        [6., 5.]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1429a64-c8a2-46b0-b216-a0cc4584b5e7",
   "metadata": {},
   "source": [
    "Voila !! c now has averages of rows in b till that index by matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0421d5-42a9-4b64-9857-1b2cc4b070c7",
   "metadata": {},
   "source": [
    "Going back to our x, and doing the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b63bc0d6-099c-4691-82c5-1f5dadf6f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = torch.tril(torch.ones(T,T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fed91f17-3ff9-40d1-ab44-b7ba6908ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "wei = wei/wei.sum(axis=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e671889a-2d98-4e23-a1dd-3b666208f76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2e4c483c-9091-4690-8f49-973446a87cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is the weighting tensor to ensure that each position attends to only current and past positions, with the weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0926a848-f7c9-474e-94e3-a3ac249d50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow2 = wei @ x  ## wei - T*T, x - B*T*C. Doesn't exactly match, pytorch will create an additional batch element in wei on its own\n",
    "                 ## so finally will be B*T*C"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aee6b1af-9d3a-4375-b71e-0bccde8868ad",
   "metadata": {},
   "source": [
    "xbow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f12fa696-6009-456f-a977-502a6ccc4904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16e7fda3-4b07-4da1-978c-850c7eae2bd3",
   "metadata": {},
   "source": [
    "xbow-xbow2 #3 more or less the same !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c746d34-f875-4500-87df-058408500645",
   "metadata": {},
   "source": [
    "Here's a 3rd equivalent implementation using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6472276a-dfba-452e-9d78-d5aa9bd3b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros(T,T)\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "98175baa-d4a3-4f14-911b-026edfeb72fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "03c7d47e-3497-49d6-94c7-14c8f7e464eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a matrix of 0's, with -inf for future elements . In the real implementation of self-attention\n",
    "## the futures will be -inf, but instead of 0's, you will will get floats of affinities from the query, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b79ee24c-74b0-4351-a8c9-062dfbaa5883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "wei = torch.softmax(wei, dim=1) ## This is equivalent to what we did above with torch.mean in implementation 2 !!\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "82560ce5-c24f-483e-8029-8faca78ea99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow3 = wei @ x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9498b01a-2122-4853-98f7-0c8a7353b23c",
   "metadata": {},
   "source": [
    "## xbow, xbow2, xbow3 are identical !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ee44b-c3f5-4fec-9d40-122f7afa93fd",
   "metadata": {},
   "source": [
    "## Let's now implement regular self-attention as a way for a token to get information from all other tokens  (as described in the paper attention is all you need)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1f62d04a-74a1-4673-a6de-d7bd5bc8aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,T,C = 4,8,32 ## Now we're saying the channel or embedding size is 32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "## Define a single head, with head size = 16\n",
    "head_size = 16\n",
    "\n",
    "## Now, given input x, you get key and query and value from x (1 per token for every batch)\n",
    "key = nn.Linear(C, head_size, bias=False) ## Input is B*T*C, output is B*T*head_size\n",
    "query = nn.Linear(C, head_size, bias=False) ## Input is B*T*C, output is B*T*head_size\n",
    "value = nn.Linear(C, head_size, bias=False) ## Input is B*T*C, output is B*T*head_size\n",
    "\n",
    "k = key(x) ## size is  B*T*head_size\n",
    "q = query(x) ## size is B*T*head_size\n",
    "v = value(x) ## size is B*T*head_size\n",
    "\n",
    "## Note that this is self-attention (keys, queries and values from from same source x)\n",
    "## For cross-attention ( machine translation problem where its not self-attention), the query comes from decoder \n",
    "## and key and value come from encoder\n",
    "## Given key and query for every token, we compute a T*T attention matrix for each batch (communication between tokens)\n",
    "wei = q @ k.transpose(-2,-1)  ## q is B*T*C, you want k to be transposed such that dimension becomes B*C*T to get B*T*T, so transpose second and third dimension of k for every batch ie last 2 dimensions\n",
    "wei = wei * (head_size)**(-0.5) ## Normalizing by sq root of head size\n",
    "## Now wei is B*T*T\n",
    "## For decoder architecture, we have to apply masking just like before\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\") ) ## For a encoder architecture, this step will not be there\n",
    "\n",
    "## Now we apply softmax row wise (across columns). Now we have the normalized attention weights for every batch\n",
    "wei = F.softmax(wei, dim=1) ## note that wei is of course not symmetric because of multiple reasons - query - key generated differently, masking future tokens, softmax across columns. So A->B influence is not the same as B->A influence \n",
    "\n",
    "## Now we multiple value with the attention weights\n",
    "out = wei @ v ## B*T*T * B*T*head_size = B*T*head_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3d4e167f-9a38-4afa-bbf6-03753900b3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c87c7-0f52-4f7c-9cb5-affd9deacaa4",
   "metadata": {},
   "source": [
    "1) Attention is a communication mechanism (directed) between tokens\n",
    "2) In a decoder architecture, the first node just communicates with itself, second node with itself and first node, and so on\n",
    "3) There is no inherent notion of space, so we need positional encodings added to character/word embeddings to get x\n",
    "4) No interaction across different samples of a given batch, only within each sample\n",
    "5) Encoder architectures won't have future masking - So just delete one line above (the masked_fill)\n",
    "6) This is self-attention where queries, keys and values come from same source. For cross-attention, query comes from one source, keys and values from another (for example machine translation)\n",
    "7) In the Attention is all you need paper, the dot product between query and key is divided by sq root of head size . Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0e809b55-5032-4ecb-be3e-d935feddb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assume key and query are unit normal\n",
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "w1 = k @ q.transpose(-2,-1) ## Without normalizing by sq root of head\n",
    "w2 = k @ q.transpose(-2,-1) * (head_size**(-0.5)) ## With normalizing by sq root of head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b4ab397e-71c1-45dc-ba22-000312a3abee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9279)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "47da4519-b946-49af-8a20-03610f2ec4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0447)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b9705a2a-4da8-4cc3-9aaf-325e335aaf7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.9491)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cbfdcf1b-185a-4da9-b364-d314860d9a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9968)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a4c86-e010-4d38-86dd-044acda9a5ec",
   "metadata": {},
   "source": [
    "Note above, that if w1 has variance much higher than k and q, whereas w2 is comparable\n",
    "\n",
    "Why is this important ? If the variance is higher, taking a softmax skews/sharpens the output towards a single value, restricting information flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "828b94ae-d3a9-4f99-9572-2efa27b306eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0548, 0.5621, 0.2015, 0.0012, 0.1149, 0.0012, 0.0151, 0.0491]])\n",
      "tensor([[0.1274, 0.2281, 0.1765, 0.0490, 0.1534, 0.0493, 0.0924, 0.1240]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.softmax(w1[0][0].reshape(1,8), dim=1)) ## sharpens element 2\n",
    "print(torch.softmax(w2[0][0].reshape(1,8), dim=1)) ## less sharper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f011c0-7390-4302-bbb8-19fecb329851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
